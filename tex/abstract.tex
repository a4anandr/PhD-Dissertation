% Write in only the text of your abstract, all the extra heading jargon is automatically taken care of
\begin{abstract}
This thesis broadly focuses on the development of Monte Carlo based methods for applications in nonlinear filtering and variance reduction in simulation algorithms. Poisson's equation is a central theme in stochastic optimal control and Markov chain theory. In this thesis, a particular version of the Poisson's equation associated to the Langevin diffusion is studied. 

Feedback particle filter (FPF) is a Monte Carlo based approximation to the nonlinear filter based on mean field optimal control techniques. It is known that the gradient to the solution of a particular version of the Poisson's equation plays a crucial role in the FPF. It was recently discovered that the same object can be used to reduce variance in common Markov Chain Monte Carlo (MCMC) algorithms. In the feedback particle filter, the gradient to the solution has interpretation as the innovations gain. In MCMC algorithms, it appears in the objective function to minimize the asymptotic variance of the mean estimates; optimal control variates can be computed using the approximation.

The main contributions of this thesis are as follows - a new formulation of the TD-learning algorithm, called the differential TD-learning is proposed to approximate the gradient of the solution to the Poisson's equation directly. This requires the difficult choice of an appropriate parameterized family of functions within which the approximation lies.  In addition to considering a finite dimensional family of functions, the basis selection problem is addressed by using a reproducing kernel Hilbert space (RKHS). In the RKHS setting, the objective function is represented in the form of an empirical risk minimization (ERM) problem that allows us to apply a recent variation of RKHS theory to find the optimal approximation. Both the applications are discussed in detail and illustrated with numerical experiments.
\end{abstract}
