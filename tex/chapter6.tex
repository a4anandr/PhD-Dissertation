\chapter{Conclusions and Future Work}
\label{ch:conclusions}
The objectives of the doctoral research surveyed in this dissertation may have been overly broad,  but to a large extent the goals have been met. The development of $\gradTD$ learning based algorithms was mainly motivated by its application to FPF. Prior to this dissertation, the FPF gain estimation was an open problem. Only constant gain and Galerkin approximations were used in practice. We developed a new class of differential LSTD learning algorithms, whose applications are not restricted to FPF gain approximation. The discrete-time and finite state space analog of the generic $\gradTD$ algorithm presented here has been applied to optimal control problems like speed scaling in microprocessors \cite{ctcn,devmey16a}. However, the original algorithm was inefficient both statistically and computationally. Statistically, it suffered from high variance issues, and computationally, an additional layer of complexity was introduced which required simulation of the Langevin SDE. This algorithm was also not friendly for online filtering problems as it requires simulating the SDE upto one million samples per filtering time step. \Prop{prop:lang_generator_grad} provided an important breakthrough and helped us develop a $\gradTD$-L version for the Langevin diffusion in Chapter 2. This algorithm yields a more computationally efficient method by reducing particle sizes to $N=500$ or $1000$ from $10^6$. The need to obtain a smooth approximation for the empirical posterior is also avoided. Thus, it is more ``plug and play'' in nature, when applied to a filtering problem.  

However, one major difficulty that remained unsolved is the extension to problems with higher dimensional state spaces. An appropriate choice of a parameterized family of functions is difficult without any insight about the structure of the solution. Basis-independent versions of $\gradTD$ learning algorithms were developed in an RKHS setting. Using a recent extension of the classical representer theorem that includes gradient terms in the loss function, we obtained the best approximations from within an infinite dimensional Hilbert space. The $\gradTD$-RKHS learning algorithms allow easy extensions to higher dimensions. Performance comparison of all these different methods was done in the context of gain function approximation and filtering examples. 

It was also observed that the same algorithms could be applied to minimize the asymptotic variance of MCMC algorithms. For the Langevin diffusion, the asymptotic variance minimization takes an objective function that exactly matches the framework of $\gradTD$ learning. Through recent research, we provide theoretical justification to apply the control variates methods to non-Langevin based algorithms like RWM as well. 

In spite of this, there remains several open research problems, in establishing relevant theory, developing more efficient algorithms, as well as in finding practical applications: 
\begin{romannum}
\item Investigate why the reduced complexity solution is as good as the optimal?\footnote{committee member Prof. J. Princip\'{e} conjectures that a proof of approximately optimality may not be a significant technical challenge}
\item Error analysis in machine learning, and in particular in the context of kernel methods, is an active research topic \cite{boueli02}. We think this would lead to valuable insights on choices of hyper parameters $\reg$ and $\epsy$.
\item A more thorough comparison of the various gain approximation algorithms is also needed.
\end{romannum}

In terms of algorithm development, we should aim for:
\begin{romannum}
\item Developing a $\gradTD$-RKHS algorithm with a differential regularizer. Currently, this is limited by the scope of representer theorem. 
\item Developing an algorithm based on semiparametric representer theorem, that allows the use of additional parameterized functions into the approximating class.
\end{romannum}
In terms of practical applications of the work, we need to investigate:
\begin{romannum}
\item  Real time filtering problems - Potential application to battery SOC estimation is being explored currently. 
\item Applications of FPF to control in the context of partially observed Markov decsion processes.
\end{romannum}

