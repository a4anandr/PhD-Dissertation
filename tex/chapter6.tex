\chapter{Conclusions and Future Work}
\label{ch:conclusions}
The broad objectives that this dissertation set out to address have been met. The development of $\gradTD$ learning based algorithms was mainly motivated by its application to FPF. Prior to this dissertation, the FPF gain estimation was an open problem. Only constant gain and Galerkin approximations were used in practice. We developed a new class of differential TD learning algorithms, whose applications are not restricted to FPF gain approximation. The discrete-time and finite state space analog of the generic $\gradTD$ algorithm presented here has been applied to optimal control problems like speed scaling etc. However, the original algorithm was inefficient both statistically and computationally. Statistically, it suffered from high variance issues, and computationally, an additional layer of complexity was introduced which required simulation of the Langevin SDE. This algorithm was also not friendly for online filtering problems as it requires simulating the SDE for times $~10^5$ or $10^6$ at each time step. \Prop{prop:lang_generator_grad} provided an important breakthrough and helped us develop a $\gradTD$ version exclusive for Langevin diffusion in Chapter 2. This algorithm yields a more computationally efficient method by reducing particle sizes to $N=500$ or $1000$ from $10^6$. The need to obtain a smooth approximation for the empirical posterior is also avoided. Thus, it is more ``plug and play'' in nature, when applied to a filtering problem.  

However, one major difficulty that remained unsolved is the extension to problems with higher dimensional state spaces. An appropriate choice of a parameterized family of functions is difficult without much insight about the structure of the solution. Basis-independent versions of $\gradTD$ learning algorithms were developed in an RKHS setting. Using a recent extension of the classic representer theorem that includes gradient terms in the loss function, we are able to obtain the best approximations from within an infinite dimensional Hilbert space. The $\gradTD$-RKHS learning algorithms allow easy extensions to higher dimensions. Performance comparison of all these different methods was done in the context of gain function approximation and filtering examples. 

It was also observed that the same algorithms could be applied to minimize the asymptotic variance of MCMC algorithms. For the Langevin diffusion, the asymptotic variance minimization takes an objective function that exactly fits into the framework of $\gradTD$ learning. Through recent research, we provide theoretical justification to apply the control variates methods to non-Langevin based algorithms like RWM as well. 

In spite of this, there are still open research problems, in establishing relevant theory, developing more efficient algorithms, as well as in finding practical applications: 
\begin{romannum}
\item Investigate why reduced complexity solution is as good as the optimal?
\item Error analysis of the $\gradTD$-RKHS method, which would provide valuable insights on choices of hyper parameters $\reg$ and $\epsy$.
\item A more thorough comparison of the various gain approximation algorithms is also needed.
\end{romannum}

In terms of algorithm development, we should aim for:
\begin{romannum}
\item Developing a $\gradTD$-RKHS algorithm with a differential regularizer. Currently, this is limited by the scope of representer theorem. 
\item Developing an algorithm based on semiparametric representer theorem, that allows the use of additional parameterized functions into the approximating class.
\end{romannum}
In terms of practical applications of the work, we need to investigate:
\begin{romannum}
\item  Real time filtering problem - Potential application to battery SOC estimation is being explored. 
\item Applications to control in the form of POMDPsd
\item Extensions of the results for Langevin diffusion to other MCMC algorithms.
\end{romannum}

