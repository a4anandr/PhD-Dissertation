\chapter{Applications to Nonlinear Filtering} 
\label{ch:filtering}
% introduction to the chapter
Applications to nonlinear filtering was the main motivation behind the development of the $\gradTD$ learning algorithms in Chapters \ref{ch:diff_td} and \ref{ch:rkhs}. In this chapter, the problem of nonlinear filtering and the associated theory is described in detail in \Section{s:nl_filtering_intro}. A brief survey on approximations to the nonlinear filter is provided in \Section{s:approx_nl_filter}. Feedback particle filter (FPF), which is the main focus of the dissertation is formally introduced in \Section{s:fpf}. A critical component of the FPF is the gain function, which is obtained as the gradient of the solution to Poisson's equation associated to the Langevin diffusion. A sketch of the derivation of the optimal gain function that guarantees asymptotic exactness of the FPF to the nonlinear filter is provided in \Appendix{a:fpf}. As has been already elaborated in \Chapter{ch:diff_td}, $\gradTD$ algorithms offer a natural solution to approximating the gain function. Two enhancements to the RKHS based $\gradTD$ learning algorithm are proposed to improve the performance in the FPF in \Section{s:fpf_enhanced_td}. A Galerkin-based approximation method and one based on approximating the transition kernel of the Langevin diffusion \cite{tagmeh16a}, which was developed in parallel research is also presented. Finally, \Section{s:fpf_numerics} contains a number of numerical experiments that compare the performance of the various algorithms in gain approximation and in filtering problems.  

\section{Introduction to Nonlinear Filtering} 
\label{s:nl_filtering_intro}
A preliminary introduction to nonlinear filtering was provided in \Chapter{ch:intro}. To make things more precise, we provide a more formal description of the problem here. First, we begin with a motivating application. Nonlinear filtering has its origins in tracking problems in satellite and aircraft navigation. The key goal of filtering is to obtain recursive estimates of the state of a stochastic dynamical system based on partial noisy observations. A typical example in a tracking application is the simultaneous estimation of position and velocity of a moving target. Here, position and velocity of the target constitute the state of the system and the observations are modeled as nonlinear functions of the state made in the presence of noise. More recently, filtering has found applications in diverse areas such as machine learning \cite{bishop06}, queueing networks, mathematical finance \cite{brihan08} and data assimilation problems for weather forecasting \cite{eve94}. 

A filtering problem can be formulated in continuous or discrete-time and continuous or finite state-space depending on the application of interest. 
The simplest dynamical model for filtering in discrete-time and state space is the Hidden Markov Model (HMM). In this dissertation however, we are primarily interested in a filtering problem in continuous-time with states evolving on a Euclidean space. For simplicity, let us restrict ourselves to the scalar filtering problem:
\begin{equation}
\begin{aligned}
\text{State model}: \quad & \ud X_t&=a(X_t) \ud t +\sigma_{B}\ud B_t,\\
\text{Observation model}: \quad & \ud Z_t&=c(X_t) \ud t + \sigma_{W}\ud W_t,
\label{e:cts_filtering}
\end{aligned}
\end{equation}
where $\bfmX=\{X_t\}$ is the scalar state process, $\bfmZ=\{Z_t\}$ is the scalar observation process, $a$ and $c$ are $C^{1}$ functions, and  $\bfmB= \{B_t\}$,  $\bfmW = \{W_t\}$ are mutually independent Brownian motions.   The state model describes the evolution of the hidden state of the system. The uncertainties in the state model and the external disturbances that affect the dynamics are modeled as the state noise term $\bfmB$. Indirect observations, in the form of nonlinear functions of the state corrupted by noise $\bfmW$ are available via the observation model. The goal of the filtering problem is to approximate the posterior density 
$\pr^*_t$  of $X_t$,
given the past observation history $\clZ_t\eqdef\sigma(Z_s:s\leq t)$. For any measurable set $A \subset \Re$, the posterior density $\pr^*_t$ is defined as, 
\begin{equation}
\int_A \pr^*_t(x) \ud x \eqdef \Prob\{ X_t \in A \mid \clZ_t\}.
\label{e:posterior_exact}
\end{equation}


A huge body of literature has been devoted to the study of such problems. 
The theory of nonlinear filtering has been described in \cite{kal80, baicri08}. A more accessible derivation of the nonlinear filter, from a change of measure standpoint is provided in \cite{kutsurpfi19}. In this section, without going into a lot of detail, we state some of the important results.
\subsection{Zakai and Kushner-Stratonovich equations}
Zakai's equation \cite{zak69} and Kushner-Stratonovich (K-S) \cite{kus67, str60} equations are the key results in this area. 
The Kushner-Stratonovich equation provides a stochastic PDE describing the evolution of $\pr^*_t$:
\begin{equation}
\ud \pr^*_t(x) = \generate^\dagger \pr^*_t(x) \ud t + \frac{1}{\sigma^2_W} (c(x) - \hat{c}_t) (\ud Z_t - \hat{c}_t\ud t) \pr^*_t(x),
\label{e:kushner_stratonovich}
\end{equation}
where $\generate^\dagger \pr \eqdef - (d(\pr a) / dx) + (\sigma^2_B/2) (d^2 \pr /d x^2)$ is the adjoint operator to the differential generator of the SDE describing the state model in \eqref{e:cts_filtering} and $\hac_t \eqdef \int c(x) \pr^*_t(x) dt$. 
For a nonlinear observation function $c(x)$, a moment closure problem arises in the K-S equation and hence, it cannot generally be reduced to stochastic PDEs so that they could be numerically integrated. In some cases, it is convenient to use the Zakai's equation, which is a linear stochastic PDE that describes the evolution of the unnormalized posterior density $\tilde{\pr}^*_t$: 
\begin{equation}
\ud \tilde{\pr}^*_t(x) = \generate^\dagger \tilde{\pr}^*_t(x) \ud t + \frac{1}{\sigma^2_W}  c(x) \ud Z_t \tilde{\pr}^*_t(x), 
\label{e:zakai}
\end{equation}
\anand{need the definition of $\generate^\dagger$. Is it the adjoint to the differential generator?}

\subsection{Kalman-Bucy filter}
Solution to the \cref{e:kushner_stratonovich,e:zakai} are in general, infinite dimensional. In the special case where the functions $a(x)$ and $c(x)$ are linear, i.e. $a(x) = Ax$ and $c(x) = Cx$ and the prior distribution $\pr_0$ is Gaussian, it is guaranteed that the posterior density $\pr^*_t$ also remains Gaussian for all $t$. A closed-form solution can be obtained to the filtering problem in this case. The posterior density $\pr^*_t$ is completely characterized by the conditional mean $\mu_t$ and the state covariance $\Sigma_t$. The optimal filter is given by the classical Kalman-Bucy filter \cite{kal64}, which is described by the set of \cref{e:kalman_mu,e:kalman_sigma}, for a scalar system:
\begin{align}
\text{Conditional mean:} \quad \ud \mu_t & = A \mu_t \ud t + \underbrace{\frac{\Sigma_t C}{\sigma^2_W}}_{\text{Kalman gain}} ( \ud Z_t - C \mu_t \ud t), 
\label{e:kalman_mu} \\
\text{State covariance:}  \quad  \ud \Sigma_t &= \Bigl(2 A \Sigma_t + \sigma^2_B - \frac{\Sigma^2_t C^2}{\sigma^2_W} \Bigr) \ud t. 
\label{e:kalman_sigma}
\end{align}
Here, the conditional mean $\mu_t$ evolves according to the SDE in \eqref{e:kalman_mu} and \eqref{e:kalman_sigma} is an ODE called the continuous-time Riccati equation. The state covariance $\Sigma_t$ evolves independent of the observations $Z_t$ and the conditional mean $\mu_t$ and as a result, it can be solved offline.  
The term $\frac{\Sigma_t C}{\sigma^2_W}$ is called the Kalman gain, denoted as $\Kkal$. 
\section{Approximations to the Nonlinear Filter}
\label{s:approx_nl_filter}
In a general nonlinear setting, excepting special cases like the Bene\v{s} filter \cite{ben81}, the optimal nonlinear filter cannot expressed by a finite set of parameters. \Cref{e:kushner_stratonovich,e:zakai} show that the posterior density can be computed in a recursive fashion. This leads to the development of discretization schemes to approximate the nonlinear filter. However, numerical approximations to the solution of the K-S equation using an Euler-type discretization are not accurate or robust. 

As noted in \Section{s:filtering}, typically a filtering problem can be separated into two steps - prediction step, where the posterior density $\pr^*_t$ at time $t$ is propagated according to the state model, without accounting for the current observations, and the updation step, where the estimates are corrected after receiving the latest observations. As the state dimension increases, numerical solutions to both these steps become prohibitively expensive and approximate solutions are sought. Approaches to approximating the prediction step and the correction step can be chosen independently and then combined. In a broad sense, two approaches can be taken, one where an exact solution is obtained under the assumptions of linearity and another, where a finite-dimensional approximation of the Kushner-Stratonovich equation is used. Budhiraja et al. in \cite{budchelee07} provide a comprehensive survey of approximation techniques for nonlinear filtering, with a particular focus on particle filtering based algorithms. A tutorial on a number of variants of particle filter algorithms, particularly in the discrete-time setting is given in \cite{arumasgorcla02}.
\subsection{Extended Kalman filter}
\label{s:ekf}
Extended Kalman filter (EKF) \cite{jaz70} is based on the principle of local linearization of the state and observation models around the mean $\mu_t$. Consequently, the resulting posterior densities are approximated as Gaussians. In the EKF, the conditional mean and state covariance estimates for the system in \eqref{e:cts_filtering} are governed by the following equations: 
\begin{align}
\ud \mu_t = a(\mu_t) \ud t + \frac{\Sigma_t C(\mu_t)}{\Sigma^2_W} (\ud Z_t - c(\mu_t) \ud t), \\
\ud \Sigma_t = \Bigl(2 A(\mu_t) \Sigma_t  + \sigma^2_B - \frac{\Sigma^2_t C^2(\mu_t)}{\sigma^2_W}\Bigr) \ud t, 
\label{e:ekf}
\end{align}
where $A = \frac{d a}{dx}$ and $C = \frac{d c}{dx}$. In higher dimensional state spaces, $A$ and $C$ are the Jacobian matrices. These are fairly easy to implement for moderate state dimensions. The performance of the EKF deteriorates if the state and observation models deviate significantly from linearity or if the noise variances are high and as a result, they suffer from severe divergence and instability problems. Furthermore, the EKF fails to capture the multi-modal features of the posterior distribution.
\subsection{Particle filters}
\label{e:particle}
Particle filters are Monte Carlo based approximations to the nonlinear filter \cite{doucet2000sequential}. \anand{check this reference} They belong to the second category mentioned, wherein without relying on the linearization of dynamics, the posterior is approximated using a finite number of samples called particles. They are based on the idea of sequential importance sampling (SIS). Although, the discrete-time variant of the filter is more common, the derivation of the continuous-time particle filter is provided in \cite{kutsurpfi19}. The basic idea is this: a large number of particles $\{X^i_0\}$ drawn independently from the same prior distribution $\pr^*_0$ are propagated through time according to the state model \eqref{e:cts_filtering}. Each particle is associated with an importance weight. Initialized with equal weights, the weight corresponding to each particle is updated based on the observations as follows:
\begin{equation}
% w^i_t  = \frac{1}{W_t} \exp \Bigl[ \int_0^t \frac{c(X^i_s)}{\sigma^2_W} dZ_s - \frac{1}{2} \int_0^t \frac{c^2(X^i_s)}{\sigma^2_W} ds\Bigr],
\ud w^i_t = w^i_t  \Bigl(\frac{1}{\sigma^2_W} \Bigr) (c(X^i_t)  - \bar{c}_t ) (\ud Z_t - \bar{c}_t \ud t),
\label{e:particle_wt}
\end{equation}
where $\bar{c}_t \eqdef \sum_{i=1}^N w^i_t c(X^i_t)$. The particle locations $\{X^i_t\}$ along with the importance weights $\{w^i_t\}$ are used to come up with an empirical estimate of the posterior distribution as:
\begin{equation}
\pr^*_t(x) \approx \pr^{(N)}_t(x) = \sum_{i=1}^N w^i_t \delta(x -X^i_t),
\label{e:particle_empirical}
\end{equation}
where $\delta$ is the Dirac-delta function. 

Particle filters are easy to implement and are ideally suited for a parallel computing architecture. They do not require linearization of the model or discretization of the filter SDEs and are often seen to perform better than the EKF. However, they are known to suffer from particle degeneracy, where after a few iterations, only a handful of particles remain with significant weights, thus reducing the effective sample size. A proposed remedy is frequent resampling of particles when the effective particle size falls below a predetermined threshold. After the resampling step, the importance weights of the new particles are reinitialized to be equal. Certain resampling schemes may again result in loss of diversity or sample impoverishment due to the replication of the same particles. A host of resampling schemes aimed at resolving these issues is presented in \cite{budchelee07, arumasgorcla02}.
 
\section{Feedback Particle Filter (FPF)}
\label{s:fpf}
In this section, we introduce the feedback particle filter (FPF), which is the preferred approximation of the nonlinear filter in this dissertation. In \Section{s:approx_nl_filter}, several approximations to the nonlinear filter, including the EKF and particle filters were presented. In addition to providing an overview of such techniques, one of the goals for giving a detailed exposition is to enable us to compare and contrast the similarities (and dissimilarities) of these techniques with the FPF.  
% \anand{Include block diagrams comparing FPF and KF}
\subsection{FPF gain approximation}
% \anand{Provide a gain approx in algorithmx format}
\subsection{Enhanced $\gradTD$ algorithms for FPF} 
\subsection{Galerkin-based methods}
\subsection{Markov semigroup approximation}
\subsection{Computational complexity}

\section{Numerical Experiments}
\label{s:fpf_numerics}

Two general applications are considered in this section, distinguished by the finite dimensional class of densities $\clP$.  We first consider a Gaussian mixture model for a diffusion on the real line,  and for a diffusion on the unit circle we consider a mixture of von Mises densities \cite{haspea00}.

In each case it is also necessary to define a family of gain functions $\clK = \{K_\param : \param\in\Re^d\}$.  In control applications it is often possible to use the control solution for a stylized control model to inform the choice of basis  \cite{CTCN,huachemehmeysur11}.  In the filtering context here, we do not yet have many tools for basis selection.

One approach is to consider an asymptotic regime.  For example, in the nonlinear oscillator considered in \cite{yanmehmey13} it was possible to estimate the gain function for low signal-to-noise ratio through a Taylor series expansion.  The components of this approximation are used to obtain the family $\clK$ in
\Section{section_nl_oscillator}.

The rate of convergence of the parameter depends on properties of the stochastic process $\bfPhi$ described by Langevin's diffusion \eqref{e:LE},  and the associated stochastic process that defines the eligibility vectors  in \eqref{eq_eligibility_vec}.  While a stationary version of this process exists in all of the examples considered here,  it will be seen that in some examples the sample paths take on large values, which contributes to high variance.

These issues are most pronounced when the mixture density has a very shallow ``valley'', which is precisely the situation that leads to very large values of the true gain $\kFPF$. This is illustrated in \Fig{gain_comparison} which shows two bimodal densities and their corresponding gain functions. The two densities are very similar in shape, but, $p_{1}$ results in a much higher value of gain than $p_{2}$. High magnitudes of FPF gain affect the particles that lie in this region of the state space, leading to numerical instabilities in filtering.

\begin{figure}[h]
	\begin{center}
		\Ebox{.85}{images/Chap4_Fig_gain_comparison}
		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.01$ and $w_{0}=0.1$}
		\label{gain_comparison}
	\end{center}
\end{figure}

%\iffalse
%\begin{figure}[h]
%	\begin{center}
%		\includegraphics[width=12cm]{Chap4_Fig_low_var_w0_pt01_K_900.eps}
%		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.01$}
%		\label{low_var}
%	\end{center}
%\end{figure}
%
%\begin{figure}[H]
%	\begin{center}
%		\includegraphics[width=12cm]{Chap4_Fig_low_var_w0_pt1_K_90.eps}
%		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.1$}
%		\label{high_var}
%	\end{center}
%\end{figure}
%\fi

\subsection{Gaussian mixture model}
\label{Results}

We begin with a survey of results obtained for the Gaussian mixture model (GMM).  An integer $m\ge 1$ is fixed, and a density $p\in \clP$ has the form
\begin{equation*}
\begin{aligned}
p(x)& = \sum_{i=1}^{m} w_{i} p_{i}(x) \,,
\end{aligned}
\end{equation*}
where each $p_{i}$ is a $\mathcal{N}(\mu_i,\sigma_i^2)$ Gaussian density, and the non-negative weights $\{w_i\}$ sum to one. Let $\mu$ and $\sigma^{2}$ denote the mean and the variance of $p$.
%
%\iffalse
%\begin{figure}[h]
%	\begin{center}
%		\Ebox{.85}{Chap4_Fig_gain_comparison_low_var.eps}
%		\caption{FPF gain $\kFPF$ for the given density $p$ with low variance}
%		\label{gain_comparison_1}
%	\end{center}
%\end{figure}
%
%
%\begin{figure}
%	\begin{center}
%		\Ebox{.85}{Chap4_Fig_gain_comparison_high_var.eps}
%		\caption{FPF gain $\kFPF$ for the given density $p$ with high variance}
%		\label{gain_comparison_2}
%	\end{center}
%\end{figure}
%\fi

In each of the following experiments the density $p\in\clP$ was chosen with $m=2$,
\[
p_1 = \mathcal{N}(-3, 4),\ \
p_2 = \mathcal{N}(3, 1),\ \
w_1 =0.75,\
w_2 =0.25.
\]

\subsubsection*{Linear Parameterization}

It is reasonable to search for a basis that offers flexibility where the density $p$ takes on non-negligible values.  The following $6$-dimensional basis was chosen with this idea in mind:
\begin{equation*}
\begin{aligned}
\gradbasis_{1}(x) & = q_1(x)
\, ,\qquad
&\gradbasis_{2}(x)  = q_2(x)
\\
\gradbasis_{3}(x) & = \gradbasis_{1}(x) x \, ,\qquad
&\gradbasis_{4}(x)  = \gradbasis_{2}(x) x \\
\gradbasis_{5}(x) &= 1 \,  ,\qquad &\gradbasis_{6}(x) =q(x)/p^{\rho}(x)
\end{aligned}
\end{equation*}
where $q_i$ is $\mathcal{N}(\mu_i,\gamma_i^2)$, with $\gamma_i>\sigma_i$ for each $i$. $\gamma_i=\sigma_i+1$ was used. It is not difficult to show that the true gain is nearly constant for large $x$, with $\lim_{|x|\to\infty} \kFPF(x) = \Kkal$. The limit corresponds to the Kalman-like gain $\Kkal$ obtained for the model in which $p$ is replaced by $p_1$  (the Gaussian density with the highest variance). The function $\gradbasis_{5}$ accounts for this constant asymptotic gain value.

The form of the solution to \eqref{eq_fish} includes a factor of $p^{-1}$, which may explain the behavior seen in \Fig{gain_comparison} and this motivates the inclusion of $\gradbasis_{6}$ into the basis. Here, $q$ is $\mathcal{N}(\mu,\sigma^{2})$ and $\rho$ is chosen such that $\lim_{|x|\to\infty}\gradbasis_{6}(x)$ is bounded. The class $\clK$ is defined using this basis:
% \begin{equation*}
% K_\param = \Kkal + \param^{\transpose}\gradbasis = \Kkal + \sum_{i=1}^{d}\param_{i}\gradbasis_{i}\,, \quad\param\in\Re^d.
% \end{equation*}
\begin{equation*}
K_\param = \param^{\transpose}\gradbasis = \sum_{i=1}^{d}\param_{i}\gradbasis_{i}\,, \quad\param\in\Re^d
\end{equation*}

\Fig{f:lstd} compares the actual FPF gain with the best approximation from within the parameterized family obtained using $\nabla$-LSTD learning. Although, the approximation is not tight everywhere, it is the best that is possible for the chosen basis. As expected, the accuracy is typically better at values of $x$ for which $p(x)$ is large.

\begin{figure}[h]
	\Ebox{.9}{images/Chap4_Fig_lstd}
	\caption{$\nabla$-LSTD learning for the linear parameterization.}
	\label{f:lstd}
\end{figure}

\subsubsection*{Nonlinear Parameterization}
\label{sec_nl_param}
In a nonlinear parameterization setting, the following form is chosen for the class $\clK$.
\begin{equation*}
K_\param(x) = K_{0} + \sum_{i=1}^m \xi_{i}^{\param}(x)
\end{equation*}
Of the various nonlinear parameterizations tested, the following produced the best results,
\begin{equation*}
\xi_{i}^{\param}(x) = \dfrac{a_{i}}{(x -b_{i})^2 + c_{i}^2}
\end{equation*}
The coefficients $\{a_{i}, b_{i}, c_{i} \}, K_{0}$ constitute the parameters to be estimated.
In the simulation results surveyed here  $m=3$ and hence, $\param \in \Re^{d}$ with $d=10$.
\begin{figure}[h]
	\Ebox{1}{images/Chap4_Fig_nltdB}
	\caption{Performance of nonlinear parameterization $\nabla$-LSTD}
	\label{f:nltd}
\end{figure}

Optimal nonlinear parameterization can be obtained by running $\nabla$-TD learning using stochastic approximation techniques, discussed in \Section{nl_theory}. Two such techniques were used, namely stochastic Newton Raphson and approximation with Polyak averaging\cite{bor08a}. The scalar gain term $\gamma_t$ is set to $1/(t+1)^{\beta}$. For stochastic Newton Raphson, $\beta$ is chosen to be $1$, and $\beta=0.6$ was chosen for Polyak averaging.

\Fig{f:nltd} shows the approximate gain obtained using stochastic Newton Raphson algorithm and the  exact gain values. The approximation obtained using nonlinear parameterization matches the exact gain better than the linear parameterization. \Section{asym_variance} compares the asymptotic error variances obtained using both these algorithms.

\subsection{Nonlinear oscillator}
\label{section_nl_oscillator}

We consider next the nonlinear oscillator example introduced in \cite{yanmehmey13}.
The state evolves on the unit circle, and the observations are nonlinear:
\begin{equation*}
\begin{aligned}
\ud \oscState &= \omega \ud t+\sigma_{B}\ud B_t \quad \text{mod }2\pi,
\\
\ud Z_t &= c(\oscState)\ud t+ \sigma_{W} \ud  W_t
\end{aligned}
\end{equation*}
The parameter $\omega$ is  the
mean angular velocity,  and $\bfmB$ and $\bfmW$ are mutually independent standard Brownian motions.
The observation function is
$c(\oscState)=\frac{1}{2}[1+\cos(\oscState)]$.

The feedback particle filter for this model is given by :
\begin{equation*}
\ud \oscState^{i}_t = \omega \ud t
+\sigma_{B}\ud B^{i}_t+K(\oscState^{i}_t) \ud I^i(t)  \quad  \text{mod }2\pi\, ,
\end{equation*}
with $\ud I^i(t) = \ud Z_t-\frac{1}{2}(c(\oscState_t^{i}+\hac_t))\ud t$.
The gain function $K(\oscState,t)$ at each instant $t$ is obtained as the solution to \eqref{eq_fish}:
\begin{equation*}
- U'(\oscState) K(\oscState) + K'(\oscState) = -\tilc(\oscState)  %\qquad \{\tilc =c-\int c(\oscState)p(\oscState,t)d\oscState\}
\end{equation*}
The conditional density $p(\oscState,t)$ at any instant is modeled as a mixture of von Mises densities:
\begin{equation*}
p(\oscState)=\sum_{i=1}^{N}w_{i}p_{i}(\oscState)
\end{equation*}
Each component of the mixture $p_{i}(\oscState)$ is given as follows:
\begin{equation*}
p_{i}(\oscState,t) =  \beta^{-1} \exp  \bigl( \kappa_i \cos(\oscState-\mu_{i} )    \bigr),
\end{equation*}
where $\beta$ is a normalizing constant, and $\mu_{i}$ is the mean of the density. This reduces to   a uniform density for $\kappa_{i}=0$, and the variance vanishes as $\kappa_{i}\to \infty$ \cite{haspea00}. By choosing a family of a mixture of von Mises densities, it is possible to model any form of circular density from uniform to a bimodal Gaussian mixture.
Poisson's equation can be numerically solved in the scalar case for this mixture density $p(\oscState)$, so that the gain function $\kFPF$ is numerically computed.
% \iffalse

\subsubsection*{Simulation results}
We consider a mixture of von Mises densities with the following parameters: $\mu_1 = -\pi/3$,\ \
$\mu_2 = \pi/3$,\ \
$\kappa_1=\kappa_2=3$,\ \
$w_1 =0.7$,\ \
$w_2 =0.3$.


The $\nabla$-LSTD learning algorithm is applied to this nonlinear oscillator problem. A linearly parameterized family using sines and cosines is the chosen basis. This is a reasonable choice because for a uniform density, the gain $\kFPF=-\frac{\sin \oscState}{2\sigma_{W}^{2}}$.

\begin{figure*}
	\Ebox{.9}{images/Chap4_Fig_TD_v_BE_468}
	\caption{$\nabla$-LSTD learning for $4,6$ and $8$ dimensional basis}
	\label{f:qad468}
\end{figure*}

For comparison, we also considered an approximation computed by minimizing the $p-$norm of the Bellman error:
% \begin{equation*}
\begin{align*}
\min_{\param}\|\clE(\param)\|_{p}^{2}=\min_{\param}\langle \clD h_{\param} + \tilc \rangle_{p} = \min_{\param}\|\clD h_{\param} +\tilc \|_{p}^{2}
\end{align*}
% \end{equation*}
This minimization problem can be solved using Monte-Carlo methods.

Figure \ref{f:qad468} compare the performance of the $\nabla$-LSTD learning algorithm and the Bellman error minimization algorithm with the numerically computed exact gain function for basis functions of dimensions $4$, $6$ and $8$. It can be observed that $\nabla$-LSTD learning gives a better approximation than the $BE$ minimization algorithm in regions of high values of $p(\oscState)$.




%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_4b}
%	\caption{For a 4 dimensional basis}
%	\label{f:qad4d}
%\end{figure}
%
%
%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_6b}
%	\caption{For a 6 dimensional basis}
%	\label{f:qad6d}
%\end{figure}
%
%
%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_8b}
%	\caption{For an 8 dimensional basis}
%	\label{f:qad8}
%\end{figure}

\begin{figure}[h]
	\Ebox{1}{images/Chap4_Fig_hist_err_peak_1+2}
	\caption{ Histogram of error between $\kFPF$ and $K_\param$ at $\mu_{1}$ and $\mu_{2}$ }
	\label{var_snr_1}
	\vspace{-.15cm}
\end{figure}

\subsection{Variance analysis of stochastic approximation algorithms}
\label{asym_variance}
In this section, the variances in the parameter estimates obtained using stochastic approximation with Polyak averaging and stochastic Newton Raphson are compared. Stochastic Newton Raphson and the Polyak averaging method give the same theoretical optimal asymptotic variance. We investigate the realized variance after running the algorithms for $T=10,000$ seconds.

The error measure is defined as
\begin{equation*}
\begin{aligned}
\epsilon(x) &= \kFPF(x)-K_\param(x)
\end{aligned}
\end{equation*}
The error values at the peaks of the density are of utmost interest because these are the locations where a majority of the particle population lie. \Fig{var_snr_1}  compares the variance and bias of the error evaluated at $x=\mu_{1}$ and $x=\mu_{2}$, for each algorithm. The histograms are based on $1,000$ independent trials. Both methods have an error bias of close to $4$ at $\mu_{1}$ and $0$ at $\mu_{2}$. Stochastic Newton Raphson has a more unimodal error distribution compared to Polyak averaging. It also gives a slightly lower variance.




\subsection{Parameter estimation experiment}


TD learning using the nonlinear parameterization discussed in \Section{sec_nl_param} yields good approximation of the FPF gain function. Here, we perform a simulation experiment to investigate the effectiveness of using the gain obtained from TD learning for a parameter estimation example. Consider the following process where the state $X_t$ remains at its initial value for all $t$. $X(0)$ is distributed in $[-3,3]$ with a higher probability near $\pm3$. The state-observation model is,
\begin{equation*}
\begin{aligned}
\ud X_t&=0,\quad \\
\ud Z_t&= X_t \ud t+\sigma_{W}\ud W_t
\end{aligned}
\end{equation*}
The problem reduces to parameter estimation as the state remains constant. In such applications, the sequential importance resampling (SIR) particle filter may perform poorly. The transient response of the filter, i.e., how quickly the estimates reach a given neighborhood of the actual state is of interest
\subsubsection*{Filter specification}  We assume a model with a 2-component Gaussian mixture as the prior $p(0)$. We take the modes of the initial Gaussian mixture at $\pm 3$. The standard deviations are chosen to be small, $\sigma_{1}=\sigma_{2}=1$, and let $w_{1}=w_{2}=0.5$.

The EM algorithm was used for density estimation. We compare two different implementations of the FPF: one with the exact gain and the second with the approximate gain obtained using $\nabla$-TD learning using a nonlinear parameterization with $d=10$ discussed in \Section{sec_nl_param}. If the posterior density estimate $p(t)$ changes slowly, and if the parameter values are initialized with their steady state values obtained from the previous instant, convergence of parameters is quick, and thus $\nabla$-TD learning can be run in ``real time'' for filtering.

\Fig{Chap4_Fig_FPF_EM_TD} compares the trajectories of the state estimates obtained using both these FPF versions. Both the implementations show similar filtering performance.

\begin{figure}
	\begin{center}
		\Ebox{.85}{images/Chap4_Fig_FPF_EM_TD_2}
		\caption{ State estimates from the two FPF implementations - actual and $\nabla$-TD learning }
		\label{Chap4_Fig_FPF_EM_TD}
	\end{center}
\end{figure}

%
%- Introduction to the problem
%- Theory of nonlinear filtering -Crisan, Xiong (Fokker Planck equations, Kolmogrov forward equation and Kushner-Stratonovich equation)
%- Survey on approximate filters, particle filters and its associated particle degeneracy problems
%- Introduction to FPF, a bit about its derivation
%- Gain function approximation problem - Galerkin, Markov semigroup approximation
%- Enhanced algorithms 
%- Numerical applications

\label{s:intro}
This paper introduces design techniques for nonlinear filtering based on the  feedback particle filter (FPF).   

Some background on the FPF is required before discussing the contributions of the current paper.

\subsection{Background}

Particle filter techniques are based on the following common architecture:  A family of $N$ stochastic processes represent particles, denoted by $ \{X^i_t : t\in\Re,\ 1\le i\le N  \}$, where $N$ is a large number.   The estimate of the posterior at time $t$ is given by the
empirical distribution,
\begin{equation}
\pr^{(N)}_t(A)=\frac{1}{N}\sum_{i=1}^{N} \ind \{X^i_t\in A \},\quad A\subset \Re.
\label{e:posterior_emp}
\end{equation}
In the feedback particle filter,  the particle evolution equations mimic those of the state space model:
\begin{equation}
\ud X^i_t=a(X^i_t) \ud t+\sigma_{B}\ud B^i(t)+\ud \bfmU^i(t),
\label{eq_controlled_sys}
\end{equation}
where $i=1$ to $N$, $X^i_t \in \Re$ is the state of the $i^{th}$ particle at time $t$, $N$ is the total number of particles, $\bfmU^i(t)$ is the control input applied to this particle, and $\{\bfmB^i\}$ are mutually independent standard Brownian motions. For each $i$, the control input $\bfmU^i$ is constructed so that for any $t$ and any $A\in\clB $  (Borel measurable subsets of $\Re$),
\[
\Prob\{ X^i_t \in A \mid \clZ_t\} 
=
\Prob\{ X_t \in A \mid \clZ_t\}
=
\int_A \pr^*_t(x) \ud x
\]

The particles are conditionally independent given the observations, so that for large $N$,  the approximation $p_t^{(N)}\sim p_t^*$ holds in a weak sense.    This optimal choice for  $\bfmU^i$ that is obtained in  \cite{yanmehmey13} leads to the following particle dynamics:
\begin{equation}
\ud  X^i_t  =    a(X^i_t)  \ud  t  + \sigma_{B}\ud B^i(t) + \kFPF (X^i_t)\ud I^i(t)   +  \half \sigma_{W}^{2}  \kFPF (X^i_t) \kFPF'(X^i_t) \ud t
\label{e:particle_propagation}
\end{equation}
where $\{ I^i(t) : t\ge 0\}$, is the innovations process corresponding to the $i^{th}$ particle:
\begin{equation*}
\ud I^i(t) \eqdef   \ud Z_t- \half (c(X^i_t)+\hac)\ud t,
\end{equation*}
where $\hac\eqdef \Expect [c(X_t) \mid \clZ_t ] = \int c(x) p_t^*(x)\, dx$.


There are similar Monte-Carlo implementations of the nonlinear filter in the literature, such as   Daum's particle flow filter \cite{DaumHuang10}.   See \cite{taoyang_acc14} for a survey. This paper is restricted to the scalar filtering problem. However, all of the theory is valid for multidimensional problems as well.

The FPF requires the computation of the feedback particle filter gain $\kFPF$ for each $t$. It can be expressed as $\kFPF= h'$, where $h$ is a solution to a particular version of Poisson's equation:
\begin{equation}
\clD h
\eqdef
-U' h'+ h'' = -\tilc ,
\label{eq_fish}
\end{equation}
in which $\tilc = c - \hac$,  and $U = -\log(\pr^*) $.
Both $\hac$ and $\kFPF$ are functionals of the actual posterior density $\pr^*$  which we wish to estimate.  This is resolved by using the empirical distribution \eqref{e:posterior_emp} as a surrogate for $\pr^*$.


The contribution of this work is to develop algorithms to approximate the FPF gain, along with a smooth approximation of the empirical distributions.  The starting point is the specification of a parameterized family  of smooth and continuous densities, denoted  $\clP = \{ p_\alpha : \alpha\in\Re^m\}$,  and a parameterized family of continuous gain functions $\clK = \{K_\param : \param\in\Re^d\}$.
The algorithm consists of two steps:
\begin{arabnum}
	\item Obtain $\pr\in\clP $ that most closely approximates $\pr^{(N)}$.
	\item  Obtain an approximation within $\clK$ for the FPF gain, based on the solution to Step~1.
\end{arabnum}
The choice of $\clP$ will depend on the application.
One example is a finite-dimensional class of Gaussian mixture models (GMM). An $m$-component Gaussian mixture density has the following general form:
\begin{equation*}
\begin{aligned}
p(x)& = \sum_{i=1}^{m} w_{i} p_{i}(x) \qquad  \sum_{i=1}^{m} w_{i} &=1 ,
\label{gaussian_mix}
\end{aligned}
\end{equation*}
\noindent
where each $p_{i}$ is a Gaussian density with mean $\mu_{i}$ and standard deviation $\sigma_{i}$.
The choice of $\clK$ is also problem-specific -- examples are given in \Section{s:ex}.

The density estimation problem in Step~1 is defined as follows: Given the set of $N$ particles, and a family $\clP $ of probability density functions,  find the probability density $p \in \clP $ that is most likely to have generated the given particles.  The
Expectation Maximization (EM) algorithm is one approach to obtaining the maximum a posteriori (MAP) optimal density.

The main contribution of the paper is the application of a new algorithm called differential-TD ($\nabla$-TD) learning to address Step~2.   This algorithm is used to solve the minimization problem,
\begin{equation}
\min_{\param}\|\kFPF - K_\param\|_p^{2}
=
\min_{\param} \Expect\bigl[   | \kFPF (X) - K_\param(X) |^2 \bigr],
\label{e:gradTDgoal}
\end{equation}
where $X \sim p$.

This computation involves another layer of simulation that adds very little additional complexity to the FPF. 
TD-learning algorithms are used to approximate value functions \cite{bertsi96a}.  In the context of average-cost optimal control,  the solution to Poisson's equation is known as the relative value function.   Given a parameterized family of approximate solutions $\{h_\param\}$,  the TD-learning algorithm aims to minimize $\| h - h_\param\|_p^{2}$ over all $\param$. In $\nabla$-TD learning, the objective is to minimize the $p$-norm error of the gradients:
\begin{equation}
\param^* = \argmin_{\param} \| h' - h_\param'\|_p^{2}
\label{e:gradTDobjective}
\end{equation}
Given the representation $\kFPF = h' $,  it is evident that the goal of $\nabla$-TD learning
is identical to the optimization problem \eqref{e:gradTDgoal}, in which $h$ is the solution to Poisson's equation \eqref{eq_fish} with $U=-\log(p)$.



\textit{Independent of the research reported here is the remarkable paper \cite{tagmeh16}}.  It also seeks approximations of the  feedback particle filter gain. While the approach is entirely different,   it is likely that concepts from that concurrent work can strengthen the basis-oriented approach stressed in this paper.  



The remainder of the paper is organized as follows.    \Section{section_learning} contains a review of approximation techniques for the FPF gain,  and develops several formulations of  $\nabla$-TD learning algorithm in the context of nonlinear filtering.   Results from numerical experiments are surveyed in \Section{s:ex}.    Conclusions and directions for future research are contained in \Section{section_conclusions}


\section{Differential-TD Learning for the Feedback Particle Filter}
\label{section_learning}

\notes{not useful without references or explanation: There are several tools for approximating solutions to Poisson's equation using numerical methods.}

\subsection{Function Approximation}

Recall that the FPF gain is the derivative of the solution to Poisson's equation \eqref{eq_fish}.
We first review a popular technique used to estimate $h$ \cite{yanmehmey13,yanlaumehmey16}.
Suppose that we are given a $d$-dimensional  function class
$\clH  \eqdef \{h_{\param} : \param \in \Re^d\}$ to approximate $h$, and a collection of $d$ test functions $\{\test_{l}\}_{l=1}^{d}$; Then, the Galerkin relaxation of Poisson's equation is defined to be the set of $d$ equations in the parameter $\param$:
\begin{equation}
\begin{aligned}
0 &= \langle \clD h_{\param}+\tilc, \test_{l} \rangle \\
&= \int (-U'(x) h_{\param}'(x)+ h_{\param}''(x)+\tilc (x))\test_{l}(x) \ud x,
\end{aligned}
\label{eq_weak_formulation}
\end{equation}
$ 1\leq l \leq d$.


In prior work \cite{yanmehmey13,yanlaumehmey16}, $\clH$ is considered to be a linearly parameterized family of functions of the form $\clH =\{ h_{\param} = \sum_{l=1}^{d}\param_{l}\basis_{l} : \param\in\Re^d\}$, where $\{\basis_l\}$ are called the basis functions. It is assumed that each $\basis_l$ is continuously differentiable,  with gradient denoted   $\gradbasis_{l} $. These   functions are used to define the approximation of the filter gain
\begin{equation}
K_\param = h_{\param}' = \sum_{l=1}^{d}\param_{l}\gradbasis_{l}.
\label{eq_basis_galerkin}
\end{equation}
The
Galerkin method provides a good algorithmic framework for approximating the gain. However, in general it is not easy to obtain performance guarantees.

In this paper we obtain approximations by solving  the minimum norm problem \eqref{e:gradTDgoal} using $\nabla$-TD learning.  Under general conditions we can be assured that the approximation is the minimum-norm optimal solution.   We first review a recent representation of the gain $\kFPF = h'$.



\section{Conclusions}
\label{section_conclusions}

Gain approximation for the feedback particle filter is an essential part of this approach to nonlinear filtering.  We have seen that a combination of two statistical learning techniques address this approximation problem.

There remain many open questions.   Alternatives to the EM algorithm must be explored for density approximation based on particles,  and it will be valuable to apply variance reduction techniques to improve the $\nabla$-TD learning algorithms.  Also, for real-time operation, the algorithms might be modified to take into account that the posterior densities and FPF gain evolve with known dynamics.  

Basis selection is the most pressing open problem from a practical perspective.  In terms of mathematical challenges, the most important open problem is robustness of the filter to modeling error, including the impact of an imperfect FPF gain.

