\chapter{Applications to Nonlinear Filtering} 
\label{ch:filtering}
% introduction to the chapter
Applications to nonlinear filtering was the main motivation behind the development of the $\gradTD$ learning algorithms in Chapters \ref{ch:diff_td} and \ref{ch:rkhs}. In this chapter, the problem of nonlinear filtering and the associated theory is described in detail in \Section{s:nl_filtering_intro}. A brief survey on approximations to the nonlinear filter is provided in \Section{s:approx_nl_filter}. Feedback particle filter (FPF), which is the main focus of the dissertation is formally introduced in \Section{s:fpf}. A critical component of the FPF is the gain function, which is obtained as the gradient of the solution to Poisson's equation associated to the Langevin diffusion. A sketch of the derivation of the optimal gain function that guarantees asymptotic exactness of the FPF to the nonlinear filter is provided in \Appendix{a:fpf}. As has been already elaborated in \Chapter{ch:diff_td}, $\gradTD$ algorithms offer a natural solution to approximating the gain function. Two enhancements to the RKHS based $\gradTD$ learning algorithm are proposed to improve the performance in the FPF in \Section{s:fpf_enhanced_td}. A Galerkin-based approximation method and one based on approximating the transition kernel of the Langevin diffusion \cite{tagmeh16a}, which was developed in parallel research is also presented. Finally, \Section{s:fpf_numerics} contains a number of numerical experiments that compare the performance of the various algorithms in gain approximation and in filtering problems.  

\section{Introduction to Nonlinear Filtering} 
\label{s:nl_filtering_intro}
A preliminary introduction to nonlinear filtering was provided in \Chapter{ch:intro}. A schematic diagram of a state estimator is shown in \Fig{Chap1_state_est_block}. To make things more precise, we provide a more formal description of the problem here. First, we begin with a motivating application. Nonlinear filtering has its origins in tracking problems in satellite and aircraft navigation. The key goal of filtering is to obtain recursive estimates of the state of a stochastic dynamical system based on partial noisy observations. A typical example in a tracking application is the simultaneous estimation of position and velocity of a moving target. Here, position and velocity of the target constitute the state of the system and the observations are modeled as nonlinear functions of the state made in the presence of noise. More recently, filtering has found applications in diverse areas such as machine learning \cite{bishop06}, queueing networks, mathematical finance \cite{brihan08} and data assimilation problems for weather forecasting \cite{eve94}. 

A filtering problem can be formulated in continuous or discrete-time and continuous or finite state-space depending on the application of interest. 
The simplest dynamical model for filtering in discrete-time and state space is the Hidden Markov Model (HMM). In this dissertation however, we are primarily interested in a filtering problem in continuous-time with states evolving on a Euclidean space. For simplicity, let us restrict ourselves to the scalar filtering problem:
\begin{equation}
\begin{aligned}
\text{State model}: \quad & \ud X_t&=a(X_t) \ud t +\sigma_{B}\ud B_t,\\
\text{Observation model}: \quad & \ud Z_t&=c(X_t) \ud t + \sigma_{W}\ud W_t,
\label{e:cts_filtering}
\end{aligned}
\end{equation}
where $\bfmX=\{X_t\}$ is the scalar state process, $\bfmZ=\{Z_t\}$ is the scalar observation process, $a$ and $c$ are $C^{1}$ functions, and  $\bfmB= \{B_t\}$,  $\bfmW = \{W_t\}$ are mutually independent standard Brownian motions.   The state model describes the evolution of the hidden state of the system. The uncertainties in the state model and the external disturbances that affect the dynamics are modeled as the state noise term $\bfmB$. Indirect observations, in the form of nonlinear functions of the state corrupted by noise $\bfmW$ are available via the observation model. The goal of the filtering problem is to approximate the posterior density 
$\pr^*_t$  of $X_t$,
given the past observation history $\clZ_t\eqdef\sigma(Z_s:s\leq t)$. For any measurable set $A \subset \Re$, the posterior density $\pr^*_t$ is defined as, 
\begin{equation}
\int_A \pr^*_t(x) \ud x \eqdef \Prob\{ X_t \in A \mid \clZ_t\}.
\label{e:posterior_exact}
\end{equation}


A huge body of literature has been devoted to the study of such problems. 
The theory of nonlinear filtering has been described in \cite{kal80, baicri08}. A more accessible derivation of the nonlinear filter, from a change of measure standpoint is provided in \cite{kutsurpfi19}. In this section, without going into a lot of detail, we state some of the important results.
\subsection{Zakai and Kushner-Stratonovich equations}
Zakai's equation \cite{zak69} and Kushner-Stratonovich (K-S) \cite{kus67, str60} equations are the key results in this area. 
The Kushner-Stratonovich equation provides a stochastic PDE describing the evolution of $\pr^*_t$:
\begin{equation}
\ud \pr^*_t(x) = \generate^\dagger \pr^*_t(x) \ud t + \frac{1}{\sigma^2_W} (c(x) - \hat{c}_t) (\ud Z_t - \hat{c}_t\ud t) \pr^*_t(x),
\label{e:kushner_stratonovich}
\end{equation}
where $\generate^\dagger \pr \eqdef - (d(\pr a) / dx) + (\sigma^2_B/2) (d^2 \pr /d x^2)$ is the adjoint operator to the differential generator of the SDE describing the state model in \eqref{e:cts_filtering} and $\hac_t \eqdef \int c(x) \pr^*_t(x) dt$. 
For a nonlinear observation function $c(x)$, a moment closure problem arises in the K-S equation and hence, it cannot generally be reduced to stochastic PDEs so that they could be numerically integrated. In some cases, it is convenient to use the Zakai's equation, which is a linear stochastic PDE that describes the evolution of the unnormalized posterior density $\tilde{\pr}^*_t$: 
\begin{equation}
\ud \tilde{\pr}^*_t(x) = \generate^\dagger \tilde{\pr}^*_t(x) \ud t + \frac{1}{\sigma^2_W}  c(x) \ud Z_t \tilde{\pr}^*_t(x), 
\label{e:zakai}
\end{equation}
\anand{need the definition of $\generate^\dagger$. Is it the adjoint to the differential generator?}

\subsection{Kalman-Bucy filter}
Solution to the \cref{e:kushner_stratonovich,e:zakai} are in general, infinite dimensional. In the special case where the functions $a(x)$ and $c(x)$ are linear, i.e. $a(x) = Ax$ and $c(x) = Cx$ and the prior distribution $\pr_0$ is Gaussian, it is guaranteed that the posterior density $\pr^*_t$ also remains Gaussian for all $t$. A closed-form solution can be obtained to the filtering problem in this case. The posterior density $\pr^*_t$ is completely characterized by the conditional mean $\mu_t$ and the state covariance $\Sigma_t$. The optimal filter is given by the classical Kalman-Bucy filter \cite{kal64}, which is described by the set of \cref{e:kalman_mu,e:kalman_sigma}, for a scalar system:
\begin{align}
\text{Conditional mean:} \quad \ud \mu_t & = A \mu_t \ud t + \underbrace{\frac{\Sigma_t C}{\sigma^2_W}}_{\text{Kalman gain}} ( \ud Z_t - C \mu_t \ud t), 
\label{e:kalman_mu} \\
\text{State covariance:}  \quad  \ud \Sigma_t &= \Bigl(2 A \Sigma_t + \sigma^2_B - \frac{\Sigma^2_t C^2}{\sigma^2_W} \Bigr) \ud t. 
\label{e:kalman_sigma}
\end{align}
Here, the conditional mean $\mu_t$ evolves according to the SDE in \eqref{e:kalman_mu} and \eqref{e:kalman_sigma} is an ODE called the continuous-time Riccati equation. The state covariance $\Sigma_t$ evolves independent of the observations $Z_t$ and the conditional mean $\mu_t$ and as a result, it can be solved offline.  
The term $\frac{\Sigma_t C}{\sigma^2_W}$ is called the Kalman gain, denoted as $\Kkal$. 
\section{Approximations to the Nonlinear Filter}
\label{s:approx_nl_filter}
In a general nonlinear setting, excepting special cases like the Bene\v{s} filter \cite{ben81}, the optimal nonlinear filter cannot expressed in terms of a finite set of parameters. \Cref{e:kushner_stratonovich,e:zakai} show that the posterior density can be computed in a recursive fashion. This leads to the development of discretization schemes to approximate the nonlinear filter. However, numerical approximations to the solution of the K-S equation using an Euler-type discretization are not accurate or robust. 

As noted in \Section{s:filtering}, typically a filtering problem can be separated into two steps - prediction step, where the posterior density $\pr^*_t$ at time $t$ is propagated according to the state model, without accounting for the current observations, and the correction step, where the estimates are corrected after receiving the latest observations. As the state dimension increases, numerical solutions to both these steps become prohibitively expensive and approximate solutions are sought. Approaches to approximating the prediction step and the correction step can be chosen independently and then combined. In a broad sense, two approaches can be taken, one where an exact solution is obtained under the assumptions of linearity and another, where a finite-dimensional approximation of the Kushner-Stratonovich equation is used. Budhiraja et al. in \cite{budchelee07} provide a comprehensive survey of approximation techniques for nonlinear filtering, with a particular focus on particle filtering based algorithms. A tutorial on a number of variants of particle filtering methods, particularly in the discrete-time setting is given in \cite{arumasgorcla02}.
\subsection{Extended Kalman filter}
\label{s:ekf}
Extended Kalman filter (EKF) \cite{jaz70} is based on the principle of local linearization of the state and observation models around the mean $\mu_t$. Consequently, the resulting posterior densities are approximated as Gaussians. In the EKF, the conditional mean and state covariance estimates for the system in \eqref{e:cts_filtering} are governed by the following equations: 
\begin{align}
\ud \mu_t = a(\mu_t) \ud t + \frac{\Sigma_t C(\mu_t)}{\Sigma^2_W} (\ud Z_t - c(\mu_t) \ud t), \\
\ud \Sigma_t = \Bigl(2 A(\mu_t) \Sigma_t  + \sigma^2_B - \frac{\Sigma^2_t C^2(\mu_t)}{\sigma^2_W}\Bigr) \ud t, 
\label{e:ekf}
\end{align}
where $A = \frac{d a}{dx}$ and $C = \frac{d c}{dx}$. In higher dimensional state spaces, $A$ and $C$ are the Jacobian matrices. These are fairly easy to implement for moderate state dimensions. The performance of the EKF deteriorates if the state and observation models deviate significantly from linearity or if the noise variances are high and as a result, they suffer from severe divergence and instability problems. Furthermore, the EKF fails to capture the multi-modal features of the posterior distribution.
\subsection{Particle filters}
\label{e:particle}
Particle filters are Monte Carlo based approximations to the nonlinear filter \cite{doucet2000sequential}. \anand{check this reference} They belong to the second category mentioned, wherein without relying on the linearization of dynamics, the posterior is approximated using a finite number of samples called particles. They are based on the idea of sequential importance sampling (SIS). Although, the discrete-time variant of the filter is more common, the derivation of the continuous-time particle filter is provided in \cite{kutsurpfi19}. The basic idea is this: a large number of particles $\{X^i_0\}$ drawn independently from the same prior distribution $\pr^*_0$ are propagated through time according to the state model \eqref{e:cts_filtering}. Each particle is associated with an importance weight. Initialized with equal weights, the weight corresponding to each particle is updated based on the observations as follows:
\begin{equation}
% w^i_t  = \frac{1}{W_t} \exp \Bigl[ \int_0^t \frac{c(X^i_s)}{\sigma^2_W} dZ_s - \frac{1}{2} \int_0^t \frac{c^2(X^i_s)}{\sigma^2_W} ds\Bigr],
\ud w^i_t = w^i_t  \Bigl(\frac{1}{\sigma^2_W} \Bigr) (c(X^i_t)  - \bar{c}_t ) (\ud Z_t - \bar{c}_t \ud t),
\label{e:particle_wt}
\end{equation}
where $\bar{c}_t \eqdef \sum_{i=1}^N w^i_t c(X^i_t)$. The particle locations $\{X^i_t\}$ along with the importance weights $\{w^i_t\}$ are used to come up with an empirical estimate of the posterior distribution as:
\begin{equation}
\pr^*_t(x) \approx \pr^{(N)}_t(x) = \sum_{i=1}^N w^i_t \delta(x -X^i_t),
\label{e:particle_empirical}
\end{equation}
where $\delta$ is the Dirac-delta function. 

Particle filters are easy to implement and are ideally suited for a parallel computing architecture. The accuracy of the estimates improves as $N$ increases. They do not require linearization of the model or discretization of the filter SDEs and are often seen to outperform the EKF in highly nonlinear examples. However, they are known to suffer from particle degeneracy, where after a few iterations, only a handful of particles remain with significant weights, thus reducing the effective sample size. A proposed remedy is frequent resampling of particles when the effective particle size falls below a predetermined threshold. After the resampling step, the importance weights of the new particles are reinitialized to be equal. Certain resampling schemes may again result in loss of diversity or sample impoverishment due to the replication of the same particles. A host of resampling schemes aimed at resolving these issues is presented in \cite{budchelee07, arumasgorcla02}.
 
\section{Feedback Particle Filter (FPF)}
\label{s:fpf}
In this section, we introduce the feedback particle filter (FPF), which is the preferred approximation of the nonlinear filter in this dissertation. In \Section{s:approx_nl_filter}, several approximations to the nonlinear filter, including the EKF and the particle filter were presented. In addition to providing an overview of such techniques, one of the purposes behind giving a detailed exposition was to enable us to compare and contrast the similarities (and dissimilarities) of these techniques with the FPF.  

FPF was first introduced in \cite{yanmehmey11} as an alternative approach to particle filtering, inspired by mean-field optimal control techniques. Along the lines of particle filtering, the FPF is constructed as a collection of $N$ particles,  each of which evolves according to a stochastic differential equation (SDE). The state evolution of the $i^\text{th}$ particle at time $t$, denoted $X_t^i$ mimics that of the state model. The initial conditions $\{X^i_0: 1 \le i\le N\}$ are assumed i.i.d., with common prior distribution $ \pr_0^*$:
\begin{equation}
\ud X^i_t = \underbrace{a(X^i_t) \ud t + \sigma_B \ud B^i_t}_{\text{Prediction}} + \underbrace{\ud U^i_t}_{\text{Correction}},
%\kFPF_t(X^i_t) \circ \underbrace{(\ud Z_t -\half[c(X^i_t) + \hac_t]\ud t)}_{\ud I_t}\, ,
\label{e:fpf_nonlin_intro}
\end{equation}
in which each $\bfmB^i$ is a standard Brownian motion, and $\hac_t := \Expect[c(X_t^i)\mid \clZ_t]$.  The primitives $\bfmB^i$ and
$\{X^j_0: j\ge 1\}$ are mutually independent, and also independent of $(\bfmX,\bfmZ)$. The particles are all coupled via the control input term $U_t^i$ corresponding to the $i^{\text{th}}$ particle.  The conditional distribution of a particle $X^i_t$ given $\clZ_t$ is given by:
\begin{equation}
\int_A \pr_t(x) \ud x = P\{X^i_t \in A \mid \clZ_t\},
\label{e:fpf_posterior}
\end{equation}
and an empirical approximation of $\pr_t$ is obtained as:
\begin{equation}
\pr^{(N)}_t(A) \eqdef \sum_{i=1}^N \ind_A\{X^i_t \in A\}.
\label{e:fpf_emp_posterior}
\end{equation}
The particles are conditionally independent given the observations, so that for large $N$,  the approximation $\pr_t^{(N)}\sim \pr_t^*$ holds in a weak sense.    

The main difference in \eqref{e:fpf_emp_posterior} compared to the estimate in \eqref{e:particle_empirical} is that all the particles are weighted equally in the FPF. Similar unweighted particle filter approaches were proposed in \cite{mitnew04,crixio05}. Daum et al. discuss the limitations of the conventional particle filter and present an unweighted filtering scheme for the continuous-discrete time case, called the information flow filter. See \cite{taoyang_acc14} for a survey of these algorithms. 

It may also be noted that prediction and correction operations are performed in a single step in \eqref{e:fpf_nonlin_intro}. The correction step is implemented via the control input term $U^i_t$. For each $i$, the control input $\bfmU^i$ is constructed so that for any $t$ and any $A\in\clB $  (Borel measurable subsets of $\Re$),
\[
\Prob\{ X^i_t \in A \mid \clZ_t\} 
=
\Prob\{ X_t \in A \mid \clZ_t\}
=
\int_A \pr^*_t(x) \ud x
\] 
The problem of choosing the optimal $U^i_t$ such that the posterior density $\pr$ coincides with the true posterior $\pr^*_t$ is cast as an optimal control problem in \cite{yanmehmey11,yanmehmey13}. The Kullback-Leibler divergence between the true posterior $\pr^*$ and the FPF mean-field estimate $\pr$ is used as the cost function. An explicit formula for $U^i_t$ is derived in \cite{yanmehmey13}. This input is used to control the dynamics of the $i^{\text{th}}$ particle as:
\begin{equation}
\ud X^i_t = a(X^i_t) \ud t + \sigma_B \ud B^i_t + \underbrace{\kFPF_t(X^i_t) \ud I^i_t + \Omega(X^i_t,t) \ud t}_{\text{optimal control input }U^i_t}, 
\label{e:fpf_optimal_input}
\end{equation} 
where $\Omega(x,t)\eqdef \frac{1}{2} \sigma^2_W \kFPF_t(x) \frac{\partial \kFPF_t(x)}{\partial x}$ and $\{I^i_t :  t \geq 0 \}$ is analogous to the innovations process corresponding to the $i^{\text{th}}$ particle,
\begin{equation}
\ud I^i_t \eqdef \ud Z_t - \frac{1}{2} (c(X^i_t) + \hat{c}_t) \ud t,
\label{e:fpf_innovations}
\end{equation}
with $\hat{c}_t \eqdef \Expect[c(X^i_t)\mid \clZ_t] = \int c(x) \pr_t^*(x)\, dx$ and approximated using $\hat{c}^{(N)}_t  \eqdef \frac{1}{N} \sum_{i=1}^N c(X^i_t)$. The filter is expressed in the Stratonovich form as:
\begin{equation}
\ud X^i_t =  a(X^i_t) \ud t + \sigma_B \ud B^i_t + \kFPF_t(X^i_t) \circ \ud I^i_t,
\label{e:fpf_stratonovich}
\end{equation}
where, the symbol ``$\circ$'' indicates  that the SDE is in Stratonovich form. The main subject of interest in this dissertation is the gain function $\kFPF_t$, that multiplies the innovations term $I_t$ in \eqref{e:fpf_stratonovich}. A detailed description of the gain function is provided next in \Section{s:fpf_gain}.
% \anand{Include block diagrams comparing FPF and KF}
\subsection{FPF gain function}
\label{s:fpf_gain}
It is evident from \eqref{e:fpf_stratonovich}, that the FPF exhibits a gain $\times$ innovation error structure that is a hallmark feature of the Kalman-Bucy filter. This feedback control structure was also present in the K-S equation \eqref{e:kushner_stratonovich} describing the nonlinear filter, but conspicuously absent in the standard formulation of the particle filter \anand{bootstrap PF?}. The FPF restores this feedback structure, which contributes to the robustness of the filter via the self-correcting property.  The block diagrams in \Fig{fig:fpf_kalman} illustrate the similarity in structure of the FPF with the Kalman filter. However, unlike the Kalman-Bucy filter where the gain is a constant, the FPF gain function $\kFPF_t$ depends on the state $X_t$. For a linear-Gaussian system, the optimal gain has been shown to coincide with the Kalman gain. 
\begin{figure}[h]
	\begin{center}
		\includegraphics[width = 7in]{images/Chap4_FPF_Kalman}
		\caption{Schematic block diagrams comparing the Kalman filter and the feedback particle filter (FPF) \cite{yanmehmey13}}
		\label{fig:fpf_kalman}
	\end{center}
\end{figure}

To implement the FPF, the gain function $\kFPF_t$ needs to be computed for each $t$. To simplify notation, consider a fixed time $t\ge 0$,  and suppress dependency on $t$.  In particular, let $\kFPF$ denote the gain function that appears as  $ \kFPF_t$  in \eqref{e:fpf_nonlin_intro},  and let $\pr$ denote the conditional density $\pr_t$. It has been proved in \cite{yanmehmey13} that the optimal gain is obtained as the solution to the following Euler-Lagrange boundary value problem (EL-BVP):
\begin{equation}
-\frac{\partial}{\partial x} \Bigl( \frac{1}{\pr(x)} \frac{\partial}{\partial x} \{\pr(x) \kFPF(x)\}\Bigr) = \frac{1}{\sigma_W^2} c'(x), 
\label{e:fpf_el_bvp}
\end{equation}
where $c'(x) = \frac{\partial c }{\partial x}$. Denoting $\pot=-\log(\pr)\in C^1$ and assuming its gradient is globally Lipschitz continuous, it is easy to show that the EL-BVP in \eqref{e:fpf_el_bvp} takes the standard form of Poisson's equation for the Langevin diffusion \eqref{e:poissons}.
\begin{equation}
\generate h = -\nabla \pot \cdot \nabla h + \Delta h  = -\tilc, \qquad \tilc = c - \int c(x) \pr(x)\, \ud x, 
\end{equation}
where the gain $\kFPF$ coincides with the gradient of $h$:
\begin{equation}
\kFPF (x) = \nabla h(x)\, ,  \quad x\in\state\, .
\label{e:fpf_k_gradient}
\end{equation}
The formula for the gain is thus obtained via a solution to the Poisson's equation:
Conditions on $U$ and $c$ under which the solution $h$ and its gradient exists has been discussed in \Chapter{ch:diff_td}. In the remainder, it is assumed that these regularity conditions are satisfied and that the gain $\kFPF$ exists and is unique. Theorem 3.3 in \cite{yanmehmey13} states that provided the prior densities match, i.e. $\pr^*_0(x) = \pr_0(x)$, the FPF in \eqref{e:fpf_stratonovich} with the gain $\kFPF$ obtained as a solution to \eqref{e:fpf_el_bvp} is consistent with the optimal nonlinear filter \eqref{e:kushner_stratonovich}, i.e.
\begin{equation}
\pr_t(x) = \pr^*_t(x) 
\end{equation}
An estimate of $\pr_t$ given by the empirical distribution of the particles $\pr^{(N)}_t$ approximates $\pr^*_t$ as $N \to \infty$. 
\section{FPF gain approximation}
The major challenge in the implementation of the FPF is the computation of the gain function. The exact computation of $\kFPF$ is intractable outside of these particular cases. 
\begin{romannum}
\item When $\pot$ is quadratic and $c$ is linear,  then $\kFPF$ is a constant independent of $x$, and can be interpreted as a Kalman gain \cite{yanlaumehmey13}. 

\item In the general scalar case, when $d=1$, $\kFPF$ has an explicit solution:
\begin{equation}
\kFPF(x) = - \frac{1}{\pr(x)} \int_{-\infty}^{x} (c(y) - \hat{c}) \pr(y) \ud y.
\label{e:kfpf1d}
\end{equation}
\end{romannum}

This motivates approximation techniques to obtain an estimate of $\kFPF$ that is optimal in some meaningful metric. In the first part of this section, we discuss Galerkin-based gain approximation algorithms. An algorithm based on approximation of the transition kernel of the Langevin diffusion was developed in parallel research by Taghvaei et al. \cite{tagmeh16} and error analysis of this method was studied in \cite{tagmehmey17}. This algorithm has been used in this dissertation as an important benchmark for comparing the performance in numerical simulations. A short review is provided in \Section{s:coifman}. 

It is clear from \eqref{e:fpf_k_gradient}, that the objective function used in \eqref{e:gradTDobjective}  
As motivated in Chapters \ref{ch:diff_td} and \ref{ch:rkhs}, approximation algorithms based on $\gradTD$ learning form the core topic of this dissertation. Two refinements of the RKHS based $\gradTD$ learning algorithm are proposed in \Section{s:fpf_rkhs_improvements}.

\subsection{Galerkin-based methods}
\label{s:galerkin}

Recall that the FPF gain is the derivative of the solution to Poisson's equation \eqref{eq_fish}.
We first review a popular technique used to estimate $h$ \cite{yanmehmey13,yanlaumehmey16}.
Suppose that we are given a $d$-dimensional  function class
$\clH  \eqdef \{h_{\param} : \param \in \Re^d\}$ to approximate $h$, and a collection of $d$ test functions $\{\test_{l}\}_{l=1}^{d}$; Then, the Galerkin relaxation of Poisson's equation is defined to be the set of $d$ equations in the parameter $\param$:
\begin{equation}
\begin{aligned}
0 &= \langle \clD h_{\param}+\tilc, \test_{l} \rangle \\
&= \int (-U'(x) h_{\param}'(x)+ h_{\param}''(x)+\tilc (x))\test_{l}(x) \ud x,
\end{aligned}
\label{eq_weak_formulation}
\end{equation}
$ 1\leq l \leq d$.


In prior work \cite{yanmehmey13,yanlaumehmey16}, $\clH$ is considered to be a linearly parameterized family of functions of the form $\clH =\{ h_{\param} = \sum_{l=1}^{d}\param_{l}\basis_{l} : \param\in\Re^d\}$, where $\{\basis_l\}$ are called the basis functions. It is assumed that each $\basis_l$ is continuously differentiable,  with gradient denoted   $\gradbasis_{l} $. These   functions are used to define the approximation of the filter gain
\begin{equation}
K_\param = h_{\param}' = \sum_{l=1}^{d}\param_{l}\gradbasis_{l}.
\label{eq_basis_galerkin}
\end{equation}
The
Galerkin method provides a good algorithmic framework for approximating the gain. However, in general it is not easy to obtain performance guarantees.

In this paper we obtain approximations by solving  the minimum norm problem \eqref{e:gradTDgoal} using $\nabla$-TD learning.  Under general conditions we can be assured that the approximation is the minimum-norm optimal solution.   We first review a recent representation of the gain $\kFPF = h'$.


% \anand{Provide a gain approx in algorithmx format}
The following \textit{constant gain approximation}  for $\kFPF$ is easily computable, and is a component of an algorithm proposed in this paper:
\[
\hakFPF^* \eqdef \argmin_{\hakFPF \in \Re^d} \| \kFPF - \hakFPF \|^2_{L^{2}}
\]
where the minimum is over deterministic vectors.   The solution is evidently the mean,  $\hakFPF^* = \Expect[\kFPF]$.
The proof of the following representation is obtained from \eqref{e:DualGrad}:
\[
\hakFPF^*_ i =  \langle \kFPF^* ,e_i \rangle_{L^2}
=
\langle \tilc ,\psi_i \rangle_{L^2}\,, \qquad 1\le i\le d\,,
\]
where $\hakFPF^*_i$ is the $i^{th}$ component of the gain, $\{e_i\}$ are the standard basis elements in $\Re^d$, and  $ \psi_i(x)\equiv x_i$.
An approximation is obtained as the empirical mean
\begin{equation}
\hakFPF^* \approx \frac{1}{N} \sum_{j=1}^N [ c(x^j) - \hac ]x^j
\label{e:gain_const_approx}
\end{equation}
This approximation has been successfully tested in \cite{tilghiomeh13}.

\subsection{Enhanced $\gradTD$ algorithms for FPF} 

\label{s:fpf_rkhs_improvements}

The online filtering problem is considered here.
It is assumed that time is sampled with constant inter-sampling time $\delta$.     The gain updates are performed at $t = n \delta$,   and use $\kFPF_n$ rather than $\kFPF_t$.  In numerical results described below,  the  SDE is approximated using the same time discretization.


\subsection{Dynamic regularization - RKHS with memory}
\label{s:RKHS_memory}



It is expected that $\kFPF_{n} = \kFPF_{t_n}  \approx \kFPF_{t_{n-1}}$ if $\delta\approx 0$.   This is the motivation for the dynamic regularization developed in this section.    Given an additional regularization parameter $\lambda_1$,  the proposed ERM is defined as in \eqref{e:erm},  with modified loss function:
\begin{equation}
\begin{aligned}
g^*_n & := \argmin_{g \in \clH} \frac{1}{N} \sum_{j=1}^N  L_n(x_n^j,g,\nabla g) + \lambda \|g\|^2_\clH
\\
L_n(x,g,\nabla g) &  \eqdef  \| \nabla g(x) \|^2 - 2 \tilc_N(x)g(x)  + \qquad + \lambda_1 \|\nabla g(x) -  \nabla g_{n-1}(x)\|^2
\end{aligned}
\label{e:erm_regularized}
%\end{aligned}
\end{equation}
The extended representer theorem again leads to a solution of the form \eqref{e:ext_rep_theorem},  and we then take
$ \kFPF_{t_n}(x^j_n) = \nabla g^*_n(x^j_n)$.


A reduced complexity approximation for the gain function at step $n$ can be obtained using \eqref{e:g_circ} as
\begin{equation}
g^*_n(.)  \eqdef \sum_{j=1}^N \beta_{j,n} \Kern(x^j_n, . ) \, .
\end{equation}
Substituting \eqref{e:grad_g_star} into \eqref{e:erm_regularized}, and using the vector/matrix notation as defined in \eqref{e:beta_2N} gives,
\begin{equation}
\beta ^*_n =
\argmin_{\beta  \in \Re^N}  \frac{1}{N} \Bigl[ (1 + \lambda_1) \beta ^\transpose \Bigl( \sum_{k=1}^d M_{x_k}^\transpose M_{x_k} \Bigr) \beta  -  \beta ^\transpose  ( 2 \lambda_1 \sum_{k=1}^d M_{x_k}^\transpose \kFPF_{n-1,k} +M_0 \vectilc  \Bigr) ] + \lambda \beta ^\transpose M_0 \beta
\end{equation}
This is a quadratic optimization problem with solution
\begin{equation}
\begin{aligned}
\beta ^*_n  &= M^{-1} b
\\
\text{with} \quad
&
M =  (1+ \lambda_1)\sum_{k=1}^d M_{x_k}^\transpose M_{x_k} + \lambda N M_0
\\
&b =  M_0 \vectilc  + \lambda_1 \sum_{k =1}^d  M_{x_k}^\transpose \kFPF_{n-1,k}
\end{aligned}
\label{e:beta_n}
\end{equation}

\subsection{Utilizing the constant gain approximation}
\label{s:RKHS_OM}



%\rd{I don't think you have motivation that you have addressed this challenge:
%The choice of the hyperparameters $\lambda$ and $\epsy$ for the RKHS based method is challenging, especially in higher dimensions as there is no knowledge about the true gain function. Hence it is ...}

The constant gain approximation \eqref{e:gain_const_approx} has been shown to work well in applications \cite{tilghiomeh13}. It is also easy to compute, so it is natural to impose the constraint at time $t=n\delta$,
\[
\nabla g =  \hakFPF^*_{t_n} +  \nabla \tilg
\]
in which $\tilg\in \clH \cap C^1$,  and the mean of  $ \nabla \tilg$ under the density $\rho_t$ is equal to zero.
It is not difficult to introduce this additional constraint in any of the ERM formulations.  The RKHS approach with this additional constraint is termed the optimal mean (OM) algorithm.


To simplify notation, dependency on $n$ (or $t$) is suppressed. The constrained optimization problem is defined as follows:
\[
\begin{aligned}
\tilg^*  \eqdef \argmin_{\tilg\in\clH} \ \  &   \| \nabla h - \hakFPF^* - \nabla \tilg \|^2_{L_2} \\
\text{s.t. }  \ \   & \langle \partial_{x_k} \tilg , 1 \rangle_{L_2} = 0, \quad     1\le  k \le d
\end{aligned}
\]
where $\hakFPF^*$ is defined in \eqref{e:gain_const_approx},  and ``$1$'' is the constant function, identically equal to unity.


The solution can be obtained by finding a saddle point for the Lagrangian,  with Lagrange multiplier $\mu  \in\Re^d$:
\begin{equation}
L(\tilg, \mu ) \eqdef   \| \nabla h - \hakFPF^* - \nabla \tilg \|^2_{L_2} +  \langle \mu ,  \nabla \tilg \rangle_{L_2}
%\tilg^* = \argmin_{g\in\clH} \| \nabla h - \hakFPF^* - \nabla g \|^2_{L_2} +  \langle \mu ,  \nabla g \rangle_{L_2} \\
\label{e:lag_dual}
\end{equation}
Expanding the quadratic, and applying \eqref{e:DualGrad} as in previous ERM formulations gives
\begin{equation}
L(\tilg, \mu ) = \| \nabla h - \hakFPF^*\|_{L_2}^2 + \|\nabla \tilg\|_{L_2}^2 - 2 \langle \tilc, \tilg \rangle_{L_2} + 2 \langle \hakFPF^*, \nabla \tilg \rangle_{L_2} +  \langle \mu ,  \nabla \tilg \rangle_{L_2}
\end{equation}
The pair $(\tilg^*,\mu^*) $ are obtained through the max-min problem:
\begin{equation}
\max_\mu \min_{\tilg}
L(\tilg, \mu )
\end{equation}
As in each previous setting, this is approximated by a regularized ERM.
An empirical saddle-point problem is defined as follows,
\begin{equation}
\begin{aligned}
(\mu^* ,  \tilg^*)  & =
\argmax_\mu  \Bigl(\argmin_{\tilg \in \clH} \Bigl[ \frac{1}{N} \sum_{i=1}^N  L(x^i,\mu, g,\nabla g) + \lambda \|g\|^2_\clH \Bigr] \Bigr) \\
L(x,\tilg,\nabla \tilg, \mu)
&= \|\nabla \tilg\, (x) \|^2    - 2   \tilc_N(x)   \tilg (x) +     \nabla \tilg \, (x) \cdot [2 \hakFPF^*   + \mu]
\end{aligned}
\end{equation}
The extended representer theorem again leads to a solution of the form \eqref{e:ext_rep_theorem} for the optimizer $ \tilg^*$.
A closed form expression is possible because this reduces to a quadratic program in $\beta^*$. An explicit solution is presented here only for the reduced complexity approximation, in which the optimization is performed over the finite-dimensional subspace \eqref{e:g_circ}. Let $\kappa$ denote the matrix whose $j$th column is equal to $K_{x_j}\pmb{1}$,  with $\pmb{1}$ the column vector consisting of ones.   A suboptimal solution over the subspace \eqref{e:g_circ} is obtained, similar to the computation leading to   \eqref{e:grad_g_star}:
\begin{equation}
\begin{aligned}
\beta ^* \eqdef
\argmin_{\beta  \in \Re^N} \frac{1}{N} & \Bigl[  \beta ^\transpose \Bigl( \sum_{k=1}^d M_{x_k}^\transpose M_{x_k} \Bigr) \beta   - 2 \beta ^\transpose  M_0 \vectilc  \\
& + 2 \beta ^\transpose \kappa \hakFPF^* + \beta ^\transpose \kappa \mu  \Bigr]+ \lambda \beta ^\transpose  M_0 \beta
\end{aligned}
\end{equation}

Taking derivatives with respect to $\beta $ and $\mu $ and equating to zero gives $N+d$ linear equations in $N+d$ unknowns \eqref{e:beta_mu}:
\spm{!!  ampersand BEFORE equal sign}
\begin{equation}
\begin{aligned}
0  &=  2 \Bigl(  \frac{1}{N}  \sum_{k=1}^d M_{x_k}^\transpose M_{x_k}   +  \lambda M_0 \Bigr) \beta ^* + \frac{ \kappa \mu ^*}{N}+  \frac{2}{N} \Bigl( \kappa \hakFPF^*  -   M_0 \vectilc \Bigr)  \\
0  & = \kappa^{\transpose} \beta^*
\label{e:beta_mu}
\end{aligned}
\end{equation}
The gain $\kFPF$ is then computed as
$
\kFPF = \hakFPF^* + \nabla \tilg^*$.

Performance of the resulting RKHS-OM algorithm is illustrated in the next section.




\subsection{Markov semigroup approximation}
\subsection{Computational complexity}

\section{Numerical Experiments}
\label{s:fpf_numerics}

Two general applications are considered in this section, distinguished by the finite dimensional class of densities $\clP$.  We first consider a Gaussian mixture model for a diffusion on the real line,  and for a diffusion on the unit circle we consider a mixture of von Mises densities \cite{haspea00}.

In each case it is also necessary to define a family of gain functions $\clK = \{K_\param : \param\in\Re^d\}$.  In control applications it is often possible to use the control solution for a stylized control model to inform the choice of basis  \cite{CTCN,huachemehmeysur11}.  In the filtering context here, we do not yet have many tools for basis selection.

One approach is to consider an asymptotic regime.  For example, in the nonlinear oscillator considered in \cite{yanmehmey13} it was possible to estimate the gain function for low signal-to-noise ratio through a Taylor series expansion.  The components of this approximation are used to obtain the family $\clK$ in
\Section{section_nl_oscillator}.

The rate of convergence of the parameter depends on properties of the stochastic process $\bfPhi$ described by Langevin's diffusion \eqref{e:LE},  and the associated stochastic process that defines the eligibility vectors  in \eqref{eq_eligibility_vec}.  While a stationary version of this process exists in all of the examples considered here,  it will be seen that in some examples the sample paths take on large values, which contributes to high variance.

These issues are most pronounced when the mixture density has a very shallow ``valley'', which is precisely the situation that leads to very large values of the true gain $\kFPF$. This is illustrated in \Fig{gain_comparison} which shows two bimodal densities and their corresponding gain functions. The two densities are very similar in shape, but, $p_{1}$ results in a much higher value of gain than $p_{2}$. High magnitudes of FPF gain affect the particles that lie in this region of the state space, leading to numerical instabilities in filtering.

\begin{figure}[h]
	\begin{center}
		\Ebox{.85}{images/Chap4_Fig_gain_comparison}
		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.01$ and $w_{0}=0.1$}
		\label{gain_comparison}
	\end{center}
\end{figure}

%\iffalse
%\begin{figure}[h]
%	\begin{center}
%		\includegraphics[width=12cm]{Chap4_Fig_low_var_w0_pt01_K_900.eps}
%		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.01$}
%		\label{low_var}
%	\end{center}
%\end{figure}
%
%\begin{figure}[H]
%	\begin{center}
%		\includegraphics[width=12cm]{Chap4_Fig_low_var_w0_pt1_K_90.eps}
%		\caption{FPF gain $K(x)$ for $p(x)$ with $w_{0}=0.1$}
%		\label{high_var}
%	\end{center}
%\end{figure}
%\fi

\subsection{Gaussian mixture model}
\label{Results}

We begin with a survey of results obtained for the Gaussian mixture model (GMM).  An integer $m\ge 1$ is fixed, and a density $p\in \clP$ has the form
\begin{equation*}
\begin{aligned}
p(x)& = \sum_{i=1}^{m} w_{i} p_{i}(x) \,,
\end{aligned}
\end{equation*}
where each $p_{i}$ is a $\mathcal{N}(\mu_i,\sigma_i^2)$ Gaussian density, and the non-negative weights $\{w_i\}$ sum to one. Let $\mu$ and $\sigma^{2}$ denote the mean and the variance of $p$.
%
%\iffalse
%\begin{figure}[h]
%	\begin{center}
%		\Ebox{.85}{Chap4_Fig_gain_comparison_low_var.eps}
%		\caption{FPF gain $\kFPF$ for the given density $p$ with low variance}
%		\label{gain_comparison_1}
%	\end{center}
%\end{figure}
%
%
%\begin{figure}
%	\begin{center}
%		\Ebox{.85}{Chap4_Fig_gain_comparison_high_var.eps}
%		\caption{FPF gain $\kFPF$ for the given density $p$ with high variance}
%		\label{gain_comparison_2}
%	\end{center}
%\end{figure}
%\fi

In each of the following experiments the density $p\in\clP$ was chosen with $m=2$,
\[
p_1 = \mathcal{N}(-3, 4),\ \
p_2 = \mathcal{N}(3, 1),\ \
w_1 =0.75,\
w_2 =0.25.
\]

\subsubsection*{Linear Parameterization}

It is reasonable to search for a basis that offers flexibility where the density $p$ takes on non-negligible values.  The following $6$-dimensional basis was chosen with this idea in mind:
\begin{equation*}
\begin{aligned}
\gradbasis_{1}(x) & = q_1(x)
\, ,\qquad
&\gradbasis_{2}(x)  = q_2(x)
\\
\gradbasis_{3}(x) & = \gradbasis_{1}(x) x \, ,\qquad
&\gradbasis_{4}(x)  = \gradbasis_{2}(x) x \\
\gradbasis_{5}(x) &= 1 \,  ,\qquad &\gradbasis_{6}(x) =q(x)/p^{\rho}(x)
\end{aligned}
\end{equation*}
where $q_i$ is $\mathcal{N}(\mu_i,\gamma_i^2)$, with $\gamma_i>\sigma_i$ for each $i$. $\gamma_i=\sigma_i+1$ was used. It is not difficult to show that the true gain is nearly constant for large $x$, with $\lim_{|x|\to\infty} \kFPF(x) = \Kkal$. The limit corresponds to the Kalman-like gain $\Kkal$ obtained for the model in which $p$ is replaced by $p_1$  (the Gaussian density with the highest variance). The function $\gradbasis_{5}$ accounts for this constant asymptotic gain value.

The form of the solution to \eqref{eq_fish} includes a factor of $p^{-1}$, which may explain the behavior seen in \Fig{gain_comparison} and this motivates the inclusion of $\gradbasis_{6}$ into the basis. Here, $q$ is $\mathcal{N}(\mu,\sigma^{2})$ and $\rho$ is chosen such that $\lim_{|x|\to\infty}\gradbasis_{6}(x)$ is bounded. The class $\clK$ is defined using this basis:
% \begin{equation*}
% K_\param = \Kkal + \param^{\transpose}\gradbasis = \Kkal + \sum_{i=1}^{d}\param_{i}\gradbasis_{i}\,, \quad\param\in\Re^d.
% \end{equation*}
\begin{equation*}
K_\param = \param^{\transpose}\gradbasis = \sum_{i=1}^{d}\param_{i}\gradbasis_{i}\,, \quad\param\in\Re^d
\end{equation*}

\Fig{f:lstd} compares the actual FPF gain with the best approximation from within the parameterized family obtained using $\nabla$-LSTD learning. Although, the approximation is not tight everywhere, it is the best that is possible for the chosen basis. As expected, the accuracy is typically better at values of $x$ for which $p(x)$ is large.

\begin{figure}[h]
	\Ebox{.9}{images/Chap4_Fig_lstd}
	\caption{$\nabla$-LSTD learning for the linear parameterization.}
	\label{f:lstd}
\end{figure}

\subsubsection*{Nonlinear Parameterization}
\label{sec_nl_param}
In a nonlinear parameterization setting, the following form is chosen for the class $\clK$.
\begin{equation*}
K_\param(x) = K_{0} + \sum_{i=1}^m \xi_{i}^{\param}(x)
\end{equation*}
Of the various nonlinear parameterizations tested, the following produced the best results,
\begin{equation*}
\xi_{i}^{\param}(x) = \dfrac{a_{i}}{(x -b_{i})^2 + c_{i}^2}
\end{equation*}
The coefficients $\{a_{i}, b_{i}, c_{i} \}, K_{0}$ constitute the parameters to be estimated.
In the simulation results surveyed here  $m=3$ and hence, $\param \in \Re^{d}$ with $d=10$.
\begin{figure}[h]
	\Ebox{1}{images/Chap4_Fig_nltdB}
	\caption{Performance of nonlinear parameterization $\nabla$-LSTD}
	\label{f:nltd}
\end{figure}

Optimal nonlinear parameterization can be obtained by running $\nabla$-TD learning using stochastic approximation techniques, discussed in \Section{nl_theory}. Two such techniques were used, namely stochastic Newton Raphson and approximation with Polyak averaging\cite{bor08a}. The scalar gain term $\gamma_t$ is set to $1/(t+1)^{\beta}$. For stochastic Newton Raphson, $\beta$ is chosen to be $1$, and $\beta=0.6$ was chosen for Polyak averaging.

\Fig{f:nltd} shows the approximate gain obtained using stochastic Newton Raphson algorithm and the  exact gain values. The approximation obtained using nonlinear parameterization matches the exact gain better than the linear parameterization. \Section{asym_variance} compares the asymptotic error variances obtained using both these algorithms.

\subsection{Nonlinear oscillator}
\label{section_nl_oscillator}

We consider next the nonlinear oscillator example introduced in \cite{yanmehmey13}.
The state evolves on the unit circle, and the observations are nonlinear:
\begin{equation*}
\begin{aligned}
\ud \oscState &= \omega \ud t+\sigma_{B}\ud B_t \quad \text{mod }2\pi,
\\
\ud Z_t &= c(\oscState)\ud t+ \sigma_{W} \ud  W_t
\end{aligned}
\end{equation*}
The parameter $\omega$ is  the
mean angular velocity,  and $\bfmB$ and $\bfmW$ are mutually independent standard Brownian motions.
The observation function is
$c(\oscState)=\frac{1}{2}[1+\cos(\oscState)]$.

The feedback particle filter for this model is given by :
\begin{equation*}
\ud \oscState^{i}_t = \omega \ud t
+\sigma_{B}\ud B^{i}_t+K(\oscState^{i}_t) \ud I^i(t)  \quad  \text{mod }2\pi\, ,
\end{equation*}
with $\ud I^i(t) = \ud Z_t-\frac{1}{2}(c(\oscState_t^{i}+\hac_t))\ud t$.
The gain function $K(\oscState,t)$ at each instant $t$ is obtained as the solution to \eqref{eq_fish}:
\begin{equation*}
- U'(\oscState) K(\oscState) + K'(\oscState) = -\tilc(\oscState)  %\qquad \{\tilc =c-\int c(\oscState)p(\oscState,t)d\oscState\}
\end{equation*}
The conditional density $p(\oscState,t)$ at any instant is modeled as a mixture of von Mises densities:
\begin{equation*}
p(\oscState)=\sum_{i=1}^{N}w_{i}p_{i}(\oscState)
\end{equation*}
Each component of the mixture $p_{i}(\oscState)$ is given as follows:
\begin{equation*}
p_{i}(\oscState,t) =  \beta^{-1} \exp  \bigl( \kappa_i \cos(\oscState-\mu_{i} )    \bigr),
\end{equation*}
where $\beta$ is a normalizing constant, and $\mu_{i}$ is the mean of the density. This reduces to   a uniform density for $\kappa_{i}=0$, and the variance vanishes as $\kappa_{i}\to \infty$ \cite{haspea00}. By choosing a family of a mixture of von Mises densities, it is possible to model any form of circular density from uniform to a bimodal Gaussian mixture.
Poisson's equation can be numerically solved in the scalar case for this mixture density $p(\oscState)$, so that the gain function $\kFPF$ is numerically computed.
% \iffalse

\subsubsection*{Simulation results}
We consider a mixture of von Mises densities with the following parameters: $\mu_1 = -\pi/3$,\ \
$\mu_2 = \pi/3$,\ \
$\kappa_1=\kappa_2=3$,\ \
$w_1 =0.7$,\ \
$w_2 =0.3$.


The $\nabla$-LSTD learning algorithm is applied to this nonlinear oscillator problem. A linearly parameterized family using sines and cosines is the chosen basis. This is a reasonable choice because for a uniform density, the gain $\kFPF=-\frac{\sin \oscState}{2\sigma_{W}^{2}}$.

\begin{figure*}
	\Ebox{.9}{images/Chap4_Fig_TD_v_BE_468}
	\caption{$\nabla$-LSTD learning for $4,6$ and $8$ dimensional basis}
	\label{f:qad468}
\end{figure*}

For comparison, we also considered an approximation computed by minimizing the $p-$norm of the Bellman error:
% \begin{equation*}
\begin{align*}
\min_{\param}\|\clE(\param)\|_{p}^{2}=\min_{\param}\langle \clD h_{\param} + \tilc \rangle_{p} = \min_{\param}\|\clD h_{\param} +\tilc \|_{p}^{2}
\end{align*}
% \end{equation*}
This minimization problem can be solved using Monte-Carlo methods.

Figure \ref{f:qad468} compare the performance of the $\nabla$-LSTD learning algorithm and the Bellman error minimization algorithm with the numerically computed exact gain function for basis functions of dimensions $4$, $6$ and $8$. It can be observed that $\nabla$-LSTD learning gives a better approximation than the $BE$ minimization algorithm in regions of high values of $p(\oscState)$.




%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_4b}
%	\caption{For a 4 dimensional basis}
%	\label{f:qad4d}
%\end{figure}
%
%
%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_6b}
%	\caption{For a 6 dimensional basis}
%	\label{f:qad6d}
%\end{figure}
%
%
%\begin{figure}
%	\Ebox{.85}{Chap4_Fig_TD_v_BE_8b}
%	\caption{For an 8 dimensional basis}
%	\label{f:qad8}
%\end{figure}

\begin{figure}[h]
	\Ebox{1}{images/Chap4_Fig_hist_err_peak_1+2}
	\caption{ Histogram of error between $\kFPF$ and $K_\param$ at $\mu_{1}$ and $\mu_{2}$ }
	\label{var_snr_1}
	\vspace{-.15cm}
\end{figure}

\subsection{Variance analysis of stochastic approximation algorithms}
\label{asym_variance}
In this section, the variances in the parameter estimates obtained using stochastic approximation with Polyak averaging and stochastic Newton Raphson are compared. Stochastic Newton Raphson and the Polyak averaging method give the same theoretical optimal asymptotic variance. We investigate the realized variance after running the algorithms for $T=10,000$ seconds.

The error measure is defined as
\begin{equation*}
\begin{aligned}
\epsilon(x) &= \kFPF(x)-K_\param(x)
\end{aligned}
\end{equation*}
The error values at the peaks of the density are of utmost interest because these are the locations where a majority of the particle population lie. \Fig{var_snr_1}  compares the variance and bias of the error evaluated at $x=\mu_{1}$ and $x=\mu_{2}$, for each algorithm. The histograms are based on $1,000$ independent trials. Both methods have an error bias of close to $4$ at $\mu_{1}$ and $0$ at $\mu_{2}$. Stochastic Newton Raphson has a more unimodal error distribution compared to Polyak averaging. It also gives a slightly lower variance.




\subsection{Parameter estimation experiment}


TD learning using the nonlinear parameterization discussed in \Section{sec_nl_param} yields good approximation of the FPF gain function. Here, we perform a simulation experiment to investigate the effectiveness of using the gain obtained from TD learning for a parameter estimation example. Consider the following process where the state $X_t$ remains at its initial value for all $t$. $X(0)$ is distributed in $[-3,3]$ with a higher probability near $\pm3$. The state-observation model is,
\begin{equation*}
\begin{aligned}
\ud X_t&=0,\quad \\
\ud Z_t&= X_t \ud t+\sigma_{W}\ud W_t
\end{aligned}
\end{equation*}
The problem reduces to parameter estimation as the state remains constant. In such applications, the sequential importance resampling (SIR) particle filter may perform poorly. The transient response of the filter, i.e., how quickly the estimates reach a given neighborhood of the actual state is of interest
\subsubsection*{Filter specification}  We assume a model with a 2-component Gaussian mixture as the prior $p(0)$. We take the modes of the initial Gaussian mixture at $\pm 3$. The standard deviations are chosen to be small, $\sigma_{1}=\sigma_{2}=1$, and let $w_{1}=w_{2}=0.5$.

The EM algorithm was used for density estimation. We compare two different implementations of the FPF: one with the exact gain and the second with the approximate gain obtained using $\nabla$-TD learning using a nonlinear parameterization with $d=10$ discussed in \Section{sec_nl_param}. If the posterior density estimate $p(t)$ changes slowly, and if the parameter values are initialized with their steady state values obtained from the previous instant, convergence of parameters is quick, and thus $\nabla$-TD learning can be run in ``real time'' for filtering.

\Fig{Chap4_Fig_FPF_EM_TD} compares the trajectories of the state estimates obtained using both these FPF versions. Both the implementations show similar filtering performance.

\begin{figure}
	\begin{center}
		\Ebox{.85}{images/Chap4_Fig_FPF_EM_TD_2}
		\caption{ State estimates from the two FPF implementations - actual and $\nabla$-TD learning }
		\label{Chap4_Fig_FPF_EM_TD}
	\end{center}
\end{figure}

%
%- Introduction to the problem
%- Theory of nonlinear filtering -Crisan, Xiong (Fokker Planck equations, Kolmogrov forward equation and Kushner-Stratonovich equation)
%- Survey on approximate filters, particle filters and its associated particle degeneracy problems
%- Introduction to FPF, a bit about its derivation
%- Gain function approximation problem - Galerkin, Markov semigroup approximation
%- Enhanced algorithms 
%- Numerical applications

Both $\hac$ and $\kFPF$ are functionals of the actual posterior density $\pr^*$  which we wish to estimate.  This is resolved by using the empirical distribution \eqref{e:posterior_emp} as a surrogate for $\pr^*$.


The contribution of this work is to develop algorithms to approximate the FPF gain, along with a smooth approximation of the empirical distributions.  The starting point is the specification of a parameterized family  of smooth and continuous densities, denoted  $\clP = \{ p_\alpha : \alpha\in\Re^m\}$,  and a parameterized family of continuous gain functions $\clK = \{K_\param : \param\in\Re^d\}$.
The algorithm consists of two steps:
\begin{arabnum}
	\item Obtain $\pr\in\clP $ that most closely approximates $\pr^{(N)}$.
	\item  Obtain an approximation within $\clK$ for the FPF gain, based on the solution to Step~1.
\end{arabnum}
The choice of $\clP$ will depend on the application.
One example is a finite-dimensional class of Gaussian mixture models (GMM). An $m$-component Gaussian mixture density has the following general form:
\begin{equation*}
\begin{aligned}
p(x)& = \sum_{i=1}^{m} w_{i} p_{i}(x) \qquad  \sum_{i=1}^{m} w_{i} &=1 ,
\label{gaussian_mix}
\end{aligned}
\end{equation*}
\noindent
where each $p_{i}$ is a Gaussian density with mean $\mu_{i}$ and standard deviation $\sigma_{i}$.
The choice of $\clK$ is also problem-specific -- examples are given in \Section{s:ex}.

The density estimation problem in Step~1 is defined as follows: Given the set of $N$ particles, and a family $\clP $ of probability density functions,  find the probability density $p \in \clP $ that is most likely to have generated the given particles.  The
Expectation Maximization (EM) algorithm is one approach to obtaining the maximum a posteriori (MAP) optimal density.

The main contribution of the paper is the application of a new algorithm called differential-TD ($\nabla$-TD) learning to address Step~2.   This algorithm is used to solve the minimization problem,
\begin{equation}
\min_{\param}\|\kFPF - K_\param\|_p^{2}
=
\min_{\param} \Expect\bigl[   | \kFPF (X) - K_\param(X) |^2 \bigr],
\label{e:gradTDgoal}
\end{equation}
where $X \sim p$.

This computation involves another layer of simulation that adds very little additional complexity to the FPF. 
TD-learning algorithms are used to approximate value functions \cite{bertsi96a}.  In the context of average-cost optimal control,  the solution to Poisson's equation is known as the relative value function.   Given a parameterized family of approximate solutions $\{h_\param\}$,  the TD-learning algorithm aims to minimize $\| h - h_\param\|_p^{2}$ over all $\param$. In $\nabla$-TD learning, the objective is to minimize the $p$-norm error of the gradients:
\begin{equation}
\param^* = \argmin_{\param} \| h' - h_\param'\|_p^{2}
\label{e:gradTDobjective}
\end{equation}
Given the representation $\kFPF = h' $,  it is evident that the goal of $\nabla$-TD learning
is identical to the optimization problem \eqref{e:gradTDgoal}, in which $h$ is the solution to Poisson's equation \eqref{eq_fish} with $U=-\log(p)$.



\textit{Independent of the research reported here is the remarkable paper \cite{tagmeh16}}.  It also seeks approximations of the  feedback particle filter gain. While the approach is entirely different,   it is likely that concepts from that concurrent work can strengthen the basis-oriented approach stressed in this paper.  



The remainder of the paper is organized as follows.    \Section{section_learning} contains a review of approximation techniques for the FPF gain,  and develops several formulations of  $\nabla$-TD learning algorithm in the context of nonlinear filtering.   Results from numerical experiments are surveyed in \Section{s:ex}.    Conclusions and directions for future research are contained in \Section{section_conclusions}


\notes{not useful without references or explanation: There are several tools for approximating solutions to Poisson's equation using numerical methods.}



\section{Conclusions}
\label{section_conclusions}

Gain approximation for the feedback particle filter is an essential part of this approach to nonlinear filtering.  We have seen that a combination of two statistical learning techniques address this approximation problem.

There remain many open questions.   Alternatives to the EM algorithm must be explored for density approximation based on particles,  and it will be valuable to apply variance reduction techniques to improve the $\nabla$-TD learning algorithms.  Also, for real-time operation, the algorithms might be modified to take into account that the posterior densities and FPF gain evolve with known dynamics.  

Basis selection is the most pressing open problem from a practical perspective.  In terms of mathematical challenges, the most important open problem is robustness of the filter to modeling error, including the impact of an imperfect FPF gain.

