\chapter{Differential TD learning} 
\label{chap2_diff_td}
The main goal of this chapter is to provide the mathematical foundations of the problem that is central to the applications we consider in this thesis.
In \Section{langevin_diffusion}, a prerequisite introduction to the Langevin diffusion and its associated Poisson's equation is given. Sufficient motivation is provided as to why this equation is important to us. In \Section{diff_td_learning}, we derive the \textit{differential TD ($\gradTD$) learning} algorithm that tries to approximate the gradient of the solution to Poisson's equation directly, which forms the major contribution in this chapter. 

 %along the lines of the LSTD algorithm described in Section 11.5.2 in \cite{ctcn}

\section{Langevin Diffusion and Poisson's equation}
\label{langevin_diffusion}
In this section, we introduce the \textit{Langevin diffusion} with only the requisite amount of detail. A more elaborate discussion is reserved for \Section{langevin_mcmc}, so that the motivation is not obfuscated by the \rd{minor} technical details. 

The \textit{Langevin diffusion} may be regarded as  a $d$-dimensional gradient flow perturbed with ``noise'',  described by  the SDE,
\begin{equation}
\ud \process_t = \underbrace{- \nabla \pot(\process_t) \, \ud t}_{\text{drift}}+  \underbrace{\sqrt{2} \, \ud W_t}_{\text{diffusion}},
\label{e:langevin_cts}
\end{equation}
where $\bfmW=\{W_t : t\ge 0\}$ is a standard Brownian motion on $\Re^d$. The potential function $U:\state \to \Re$ is continuously differentiable. 
Under suitable regularity conditions, this diffusion is reversible and has a unique invariant density $\pr=e^{-\pot+\Lambda}$, where $\Lambda$ is a normalizing constant so that $\pr$ integrates to unity \cite{}. The SDE in \eqref{e:langevin_cts} can be thought of as composed of a deterministic \textit{drift} term and a stochastic \textit{diffusion} term. The intuition is that the drift term moves the process along the direction in which the density $\pr$ increases. In this sense, it is a \textit{biased} random walk. In practise, as simulating path solutions to this SDE are difficult, discretized versions of the equation based on Euler-Mauryama scheme are used, as given by,
\begin{equation}
\markovstate_n = \markovstate_{n-1} - \nabla U(\markovstate_{n-1}) \mcmcstep_{n} + \sqrt{2  \mcmcstep_{n-1}} W_{n-1} ,
\label{e:langevin_discrete}
\end{equation}
where $\{\mcmcstep_n\}_{n\geq 1}$ is a sequence of step sizes and $\{W_n\}_{n\geq 1}$ is a sequence of i.i.d. standard Gaussian random variables.  In this thesis, only implementations using a constant step size parameter $\mcmcstep_{n} \equiv \mcmcstep$ are considered. 
 
% The Markov transition kernel of the diffusion process converges to a unique invariant density $\pr=e^{-\pot+\Lambda}$ in total variation \cite{robtwe96} or Wasserstein distance \cite{bolgengui11},  where $\Lambda$ is a normalizing constant so that $\pr$ integrates to unity. 
% admits a strong solution $\{\markovstate_t\}_{t\geq0}$, 

The Langevin diffusion is associated with a \textit{differential generator} $\generate$ (also called \textit{infinitesimal generator}), which is defined as,
\begin{equation}
\begin{aligned}
\generate f &:= \lim_{t \to 0} \frac{\Expect [ f(\markovstate_t) - f(x) | \markovstate_0 = x]}{t} \\
& = -\nabla \pot \cdot \nabla f + \Delta f,\qquad f\in C^2,
\label{e:langevin_generator}
\end{aligned}
\end{equation}
where $\nabla$ denotes the gradient and $\Delta$ is the Laplacian. Let $c \colon \state \to \Re$ be a function of interest, and 
\[
\eta := \Expect_{X \sim \pr}[c(X)] =  \int c(x) \pr(x) \ud x = \langle c, 1 \rangle_{L^2}.
\]
A function $h\in C^2$ is said to be the solution to Poisson's equation with forcing function $c$ if
\begin{equation}
\generate h := - \tilc, \qquad  \tilc = c - \eta.
\label{e:poissons}
\end{equation}
Additionally, $h$ can also be expressed in the following integral form:
\begin{equation}
h(x) =\int_0^\infty \Expect[\tilc(\markovstate_t)|\markovstate_0 = x] \ud t,
\end{equation}
where $h(x)$ can be interpreted as the infinite-horizon expected normalized cost with the initial state $x$. 

The existence of a solution $h$ in a weak sense holds under very weak assumptions on $\pot$ and $c$  \cite{glymey96a,konmey12a}.   Representations for the gradient of $h$ and bounds are obtained in \cite{laumehmeyrag15,devkonmey17b}.   The existence of  a  smooth solution $h\in C^2$ has been established under stronger conditions in \cite{parver01}, subject to growth conditions on $c$ similar to those used in  \cite{glymey96a}. 

The function $h$ plays an important role in each of the applications we consider. In the feedback particle filter (FPF), the \textit{innovations gain} function $\kFPF$ is obtained as  the gradient of $h$ \cite{yanmehmey13}:
\begin{equation}
\kFPF (x) = \nabla h(x)\, ,  \quad x\in\state\, .
\label{e:fpfgain}
\end{equation}
The FPF is described in detail in \Chapter{}.

In MCMC algorithms, the quantity of interest is the \textit{asymptotic variance}. In this context, $\pr$ is the target density, and $c$ is the function whose expectation needs to be computed. The expected value is approximated using the empirical mean $\eta_N$:
\[ \eta_N := \frac{1}{N} \sum_{n=0}^{N-1} c\,(\markovstate_n),\]
where $\markovstate_n$ is obtained by sampling from a Markov chain with invariant density $\pr$. One such Markov chain is the discretization in \eqref{e:langevin_discrete}, known as the unadjusted Langevin algorithm (ULA) or Langevin Monte Carlo (LMC) \cite{}. Under general conditions, the mean estimates will obey Central Limit Theorem (CLT) of the form
\[
\sqrt{N} \Bigl( \eta_N - \eta \Bigr) \overset{d}{\to} \normal(0,\asymvar ^2),
\]
and the convergence is in distribution. Here, $\asymvar^2$ denotes the asymptotic variance that has a representation in terms of $h$ \cite{glymey96a,MT,asmgly07}. More details about the application to MCMC algorithms is in \Chapter{}. 

Computation of $h$ is typically intractable and hence, approximation techniques are required. Algorithms based on reinforcement learning provide a suitable approach. \Section{lstd_learning} provides a description of the least-squares TD (LSTD) learning algorithm. Following a brief discussion on its limitations,  the derivation of the $\gradTD$ learning is presented in \Section{difftd_learning}. 

\section{Least Squares Temporal Difference (LSTD) learning} 
In this section, a continuous-time version of least squares temporal difference (LSTD) learning is presented, to approximate the discounted-cost value function for a diffusion process, with a discount rate $\discount  >0$ \cite{brabar96}. The derivation here closely follows the one in Section 11.5.2 in \cite{ctcn}. The key difference is that the book section discusses the discrete-time case. Let $h^\discount$ be the discounted-cost value function given by,
\begin{equation}
h^\discount(x) := \int_0^\infty \exp(-\discount t )\Expect_x[c(\markovstate_t)] \ud t, 
\label{e:discount_value_fn}
\end{equation}
where $\markovstate_t$ is a continuous-time diffusion process. Langevin diffusion in \eqref{e:langevin_cts} is an example. The notation $\Expect_x[X_t]$ is shorthand for the conditional expectation $\Expect[\markovstate_t|\markovstate_0 = x]$, with $x$ as the initial state. 

Consider a parameterized family of approximations $\{h_\param : \param \in \Re^\ell\}$. In the case of a linear parameterization, where we have $\ell$ functions on $\state$ as the basis, denoted $\{\basis_i : 0 \leq i \leq \ell\}$, the parameterized family $\{h_\param\}$ becomes,
\begin{equation}
h_\param(x) := \sum_{i=1}^\ell \param_i \basis_i(x) = \param^\transpose \basis(x), \qquad x \in \state
\label{e:linear_param}
\end{equation}
The goal in LSTD learning is to minimize the approximation error in $L^2(\pr)$ norm:
\begin{equation}
\begin{aligned}
\mathcal{E}(\param) &:=  \| h^\discount - h_\param \|^2_{L^2} \\
& = \int_\state (h^\discount(x) - h_\param(x))^2 \pr(x) \ud x \\
& = \Expect_{\markovstate \sim \pr} [|h^\discount(\markovstate) - h_\param(\markovstate)|^2].
\label{e:h_norm_error}
\end{aligned}
\end{equation}
It is evident from the definition of $\mathcal{E}(\param)$ that it penalizes the error in approximation more strongly for states with larger invariant probability $\pr(x)$. As noted in \cite{ctcn}, these are the states that are visited more often and hence, a better approximation of $h^\discount$ is desired at these points on $\state$. More importantly, using the $L^2(\pr)$ norm allows the construction of an algorithm using Monte Carlo methods. 

The necessary conditions for optimality can be obtained by taking the derivative of \eqref{e:h_norm_error} with respect to $\param$ and equating it to zero, 
\[
\begin{aligned}
0 &= - \nabla_\param \mathcal{E}(\param)\\
& = 2 \|(h^\discount - h_\param) \|_{L^2} \basis \\
& = 2 \Expect_{\markovstate \sim \pr} [(h^\discount(\markovstate) - h_\param(\markovstate)) \basis(\markovstate)]
\end{aligned}
\] 
\anand{Need to verify if the exchange of derivative and expectation is valid for a nonlinear parameterization}
For the linear parameterization in \eqref{e:linear_param}, the optimal $\param^*$ admits a closed form expression:
\begin{equation}
\param^* := M^{-1} b,
\label{e:param_opt_lstd}
\end{equation}
where $M$ and $b$ are defined as,
\begin{equation}
M: = \Expect_{\markovstate \sim \pr} [\basis(\markovstate) \basis(\markovstate)^\transpose], \qquad b:= \Expect_{\markovstate \sim \pr} [h^\discount(\markovstate) \basis(\markovstate)].
\label{e:lstd_M_b}
\end{equation}
The challenge is to find an alternate representation for $b$ as it involves the unknown function $h^\discount$ that we are trying to approximate.  

A resolution is obtained using the definition of $h^\discount$ in \eqref{e:discount_value_fn} and the generalized resolvent kernel of \cite{nev72,meytwe93e,devkonmey17a}: For a measurable function $G\colon\Re\to\Re$, and measurable functions $f$ in some domain, the resolvent kernel is an operator defined as,
\begin{equation}
R_G f\, (x) := \int_0^\infty \Expect_x\Bigl[ \exp\Bigl(-\int_0^t G(\process(s))\, \rmd s \Bigr) f(\process(t))\Bigr] \ud t.
\label{e:resolvent_neveu}
\end{equation}
In  \cite{nev72,meytwe93e} it is assumed that $G>0$ everywhere. These conditions are relaxed in \cite{konmey03a,devkonmey17a}. An adjoint $R^\dagger_G$ is defined such that the following holds:
\[
\langle R_G f \, , g \rangle_{L^2} = \langle f \, , R^\dagger_G g \rangle_{L^2}, 
\]
where $f$ and $g$ are in $L^2(\pr)$. 
%In these papers, it is shown that $R_G$ is a right inverse of $[I_G -\clD]$ on some domain, i.e.,
%\begin{equation*}
%[I_G-\clD]R_G f = f,
%\end{equation*}
%where the operator $I_G$ represents multiplication by the function $G$. 

The discounted-cost value function $h^\discount$ in \eqref{e:discount_value_fn} can now be represented in terms of the resolvent kernel $R_G$ in \eqref{e:resolvent_neveu} with $G \equiv \discount$ as given by,
\begin{equation}
h^\discount(x) = R_\discount c(x).
\label{e:hgamma_resolvent}
\end{equation}
%Multiplying $h^\discount$ by $\basis$, we get,
%\begin{equation}
%\begin{aligned}
%h^\discount (x) \basis(x) &=  \Bigl(\int_0^\infty \exp(-\discount t )\Expect_x[c(\markovstate_t)] \ud t \Bigr) \basis(x) \\
%&= \int_0^\infty \exp(-\discount t) \Expect_x[c(\markovstate_t) \basis(\markovstate_t)] \ud t
%\label{e:discount_hpsi}
%\end{aligned}
% \end{equation}
Using \eqref{e:hgamma_resolvent}, $b$ admits the following transformation,
\[
\begin{aligned}
b = \Expect_{\markovstate \sim \pr} [h^\discount(\markovstate) \basis(\markovstate) ] &= \langle h^\discount, \, \basis \rangle_{L^2},\\
& = \langle R_\discount c, \,\basis \rangle_{L^2} \\
& = \langle c,\, R^\dagger_\discount \basis \rangle_{L^2},
\end{aligned}
\]
where $R^\dagger_\discount$ the adjoint of $R_\discount$. Now, all that remains is to obtain an expression for the adjoint operator $R^\dagger_\discount$ in terms of observable quantities. This is achieved by applying the stationarity property of the process $\markovstate_t$, which gives, 
\[
\Expect_{\markovstate \sim \pr}[c(\markovstate_t) \basis(\markovstate_0)] = \Expect_{\markovstate \sim \pr} [c(\markovstate_0) \basis(\markovstate_{-t})].
\] 
Rewriting $b$ in its integral form as,
\begin{equation}
\begin{aligned}
b & = \langle R_\discount c, \basis \rangle_{L^2} \\
& = \int_0^\infty \exp(-\discount t)  \Bigl(\int_{\state} \Expect[c(\markovstate_t)| \markovstate_0 = x] \basis(x)  \pr(x) \ud x \Bigr) \ud t \\
& = \int_0^\infty \exp(-\discount t)  \Bigl(\int_{\state} \Expect[c(\markovstate_t) \basis(\markovstate_0)| \markovstate_0 = x]  \pr(x) \ud x \Bigr) \ud t \\
& = \int_0^\infty \exp(-\discount t) \Expect[c(\markovstate_t) \basis(\markovstate_0)]\ud t \\
& = \int_0^\infty \exp(-\discount t) \Expect[c(\markovstate_0) \basis(\markovstate_{-t})] \ud t \qquad \text{(Applying the stationarity property of $\markovstate$)}\\
& = \Expect\Bigl[c(\markovstate_0) \int_0^\infty \exp(-\discount t) \basis(\markovstate_{-t})\ud t\Bigr]  \qquad \text{(Applying Fubini's theorem and absolute integrability)}
\label{e:lstd_resolution}
\end{aligned}
\end{equation}
It can be seen from \eqref{e:lstd_resolution} that the adjoint $R^\dagger_\discount$ operating on a function $f$ in $L^2(\pr)$ takes the form:
\[
R^\dagger_\discount f(x) \eqdef \int_0^\infty \exp(-\discount t ) f(\markovstate_{-t}) \ud t \qquad \textbf{\rd{Needs verification}}
\]
Denoting $\eligibvector_\discount$ as,
\begin{equation}
\eligibvector_\discount(r) \eqdef \int_0^\infty \exp(-\discount (t -r)) c(\markovstate_{r-t}) \ud t,
\label{e:lstd_eligib}
\end{equation}
$b$ can be written as,
\begin{equation}
b = \Expect[c(\markovstate_0) \eligibvector_\discount(0)].
\label{e:lstd_b}
\end{equation}
% \ section{Galerkin approximation}
This representation for $b$ \eqref{e:lstd_b}, combined with the representation for $M$ in \eqref{e:lstd_M_b} lends itself to application of Monte Carlo methods in their computation. Monte carlo approximations to $M$ and $b$ can be obtained using the following integral forms:
\begin{subequations}
\begin{align}
M \approx &M_T   \eqdef \frac{1}{T} \int_0^T  \basis(\markovstate_t) \basis(\markovstate_t)^\transpose \ud t
\label{e:lstd_M_T}
\\
b \approx & b_T  \eqdef \frac{1}{T} \int_0^T c(\markovstate_t) \eligibvector_\discount(t) \ud t
\label{e:lstd_b_T}
\\
\param_T &\eqdef  M_T^{-1} b_T
\label{e:lstd_theta_T}
\end{align}
\end{subequations}
Equations \cref{e:lstd_M_T,e:lstd_b_T} along with \eqref{e:lstd_eligib} summarize the LSTD algorithm. An alternate description of the algorithm can be provided in terms of the three ODEs:
\begin{subequations}
\begin{align}
\ddt \eligibvector_\discount (t) &= - \discount\, \eligibvector_\discount(t) + \basis (\markovstate_t) 
\label{e:lstd_ode_eligib} \\
\ddt b(t) &=  c(\markovstate_t)\eligibvector_\discount(t)
 \label{e:lstd_ode_b}\\
\ddt M(t) &= \basis(\markovstate_t) \basis^\transpose(\markovstate_t) 
\label{e:lstd_ode_M}
\end{align}
\end{subequations}
The system of ODEs is initialized with $\eligibvector_\discount(0), b(0) \in \Re^\ell,$ and a positive definite $\ell \times \ell$ matrix $M(0)$. 

\section{Differential TD ($\gradTD$) learning}
\label{diff_td_learning}


