\chapter{Generalization Error Bounds for least-squares regression on RKHS }%
\label{a:bousquet}
Let $Z = X \times Y$ be the space of inputs and outputs and let $S = \{z_1 = (x_1, y_1), \cdots,z_N = (x_N,y_N)\}$ be a dataset of size $N$ drawn i.i.d. from an unknown distribution $\pr_{XY}$. Let $f:X \to Y$ be a function that maps from the input space $X$ to output space $Y$. A learning algorithm tries to learn the function $f_S$ from the given dataset $S$. 

The generalization error or expected risk is defined as:
\[
R(f) := \Expect_{z \sim \pr_{XY}} [c(f,z)]
\]
The empirical error of a function $f$ measured on the training set $S$ is:
\[
R_N(f) := \frac{1}{N}\sum_{i=1}^N c(f,z_i)
\]
Our aim is to obtain bounds on the random variable $R(f_S) - R_N(f_S)$. 

If $S = \{z_1,\dots, z_{i-1},z_i,z_{i+1},\dots z_N\}$, let us define a modified training set $S_i :=\{z_1,\dots, z_{i-1},z'_i,z_{i+1},\dots,z_N\}$ where the $i^{th}$ training sample $z_i$ is replaced by $z'_i$.

Uniform stability: A notion of uniform stability is defined for the algorithm as follows. If the training set $S$ is defined as above and $S_i$ be the training set where $i^{th}$ sample is removed, then the algorithm is $\beta$-stable if the following holds:
\[
\|c(f_S, z ) - c(f_{S_i},z)\|_{\infty} \leq \beta, \qquad \forall S \in Z^N, \forall z'_i, z \in Z
\]
This condition implies stability of the algorithm in the sense that if a training sample from the original set is replaced by a new sample, the difference in cost is smaller than some constant $\beta$. 

\begin{lemma}
	For any symmetric learning algorithm we have for all $1\leq i \leq N$:
	\[
	\Expect_{S \sim \pr^N_{XY}} [R(f_S) - R_N(f_S)] = \Expect_{S,z'_i \sim \pr^{N+1}_{XY}} [c(f_S,z'_i) - c(f_{S_i}, z'_i)]
	\]
	\begin{proof}
		\[
		\Expect_{S \sim \pr^N_{XY}} [R_N(f_S)] = \frac{1}{N} \sum_{i=1}^N \Expect_{S \sim \pr^N_{XY}} [c(f_S, z_i)] = \Expect_{S \sim \pr^N_{XY}} [c(f_S, z_i)], \, \forall i \in \{1,\dots, N\}
		\]
		The above is true by symmetry and the i.i.d assumption. Now by simply renaming $z_i$ as $z'_i$,
		\[
		\Expect_{S \sim \pr^N_{XY}}[R_N(f_S)] = \Expect_{S_i \sim \pr^N_{XY}}[c(f_{S_i}, z'_i)]
		\]
		The expected risk term can be written as, 
		\[
		\Expect_{S \sim \pr^N_{XY}}[R(f_S)] = \Expect_{S,z'_i \sim \pr^{N+1}_{XY}} [c(f_S, z'_i)]
		\]
		Using this and the fact that the algorithm is $\beta$-stable, 
		\[
		\begin{aligned}
		\Expect_{S \sim \pr^N_{XY}} [R(f_S) - R_N(f_S)] &= \Expect_{S,z'_i\sim \pr^{N+1}_{XY}}[c(f_S,z'_i) - c(f_{S_i},z'_i)]\\
		&\leq \Expect_{S,z'_i\sim \pr^{N+1}_{XY}}[\beta] \\
		& = \beta
		\end{aligned}
		\]
	\end{proof}
\end{lemma}
Now, the next step is to prove that our algorithm of interest is $\beta$-stable for some value of $\beta$. 

\section{Application to regularization in Hilbert spaces}
Let $\clH$ be an RKHS induced by a kernel function $K$. Let us assume that $K$ is continuous and bounded, so that
\[
\kappa := \sup_{x \in X} \sqrt{K(x,x)} < \infty
\]
and therefore, by Cauchy-Schwarz inequality,
\begin{equation}
|f(x)| \leq \kappa \|f\|_\clH, \, \forall x \in X, \forall f \in \clH 
\label{e:cauchy_schwarz}
\end{equation}

\noindent $\sigma$-admissibility: A cost function $c(f(x),y)$ defined on $\clH \times Y$ is $\sigma$-admissible with respect to $\clH$ if $c$ is convex with respect to its first argument and the following condition holds
\[
|c(y_1, y') - c(y_2, y')| \leq \sigma |y_1 - y_2|, \, \forall y_1,y_2 \in \mathcal{D}, \forall y' \in Y,
\]
where $\mathcal{D} = \{y: \exists f \in \clH, \exists x \in X, f(x) = y\}$ is the domain of the first argument of $c$.  

\noindent \begin{theorem}
	If $l(f,z) = c(f(x),y)$ is $\sigma$-admissible with respect to $\clH$, then the learning algorithm defined by 
	\[
	A_S = \argmin_{g \in \clH} \frac{1}{N} \sum_{i=1}^N l(g, z_i) + \lambda \|g\|^2_\clH
	\]
	has uniform stability $\beta$ with respect to $l$ with
	\[
	\beta \leq \frac{\kappa^2 \sigma^2}{2 \lambda N}
	\]
	\begin{proof}
		The proof uses Lemma 20 in the paper that gives bounds on a general regularization term $N(g)$ that appears in the ERM. Here, I just state the result without the proof. 
		\begin{lemma}
			Consider two ERM formulations:
			\[
			\begin{aligned}
			\text{Problem 1:} & R_r(g) := \frac{1}{N} \sum_{j=1}^N l(g, z_j) + \lambda N(g), \\
			\text{Problem 2:} & R_r^{\textbackslash i}(g) := \frac{1}{N} \sum_{j \neq i} l(g, z_j) + \lambda N(g)
			\end{aligned}
			\]
			Let $f$ be the minimizer of $R_r$ in $\clH$ and let $f^{\textbackslash i}$ be the minimizer of $R_r^{\textbackslash i}$ in $\clH$ and denote $\Delta f = f^{\textbackslash i } - f$. Then for any $f \in [0,1]$,
			\[
			N(f) - N(f+t \Delta f) + N(f^{\textbackslash i}) - N(f^{\textbackslash i} - t \Delta f) \leq \frac{t \sigma}{\lambda N}|\Delta f(x_i)|
			\]
		\end{lemma}
		
		For regularization in the RKHS, where $N(g) = \|g\|^2_\clH$,
		\[
		2 \| \Delta f\|^2_\clH \leq \frac{\sigma}{\lambda N} | \Delta f(x_i)|
		\] 
		Using \eqref{e:cauchy_schwarz}, 
		\[
		\Delta f(x_i) \leq \kappa \|\Delta f \|_\clH ,
		\]
		so that, 
		\[
		\| \Delta f \|_\clH \leq \frac{ \sigma \kappa}{2 \lambda N}
		\]
		By the $\sigma$-admissibility of $l$,
		\[
		| l(f,z) - l(f^{\textbackslash i},z)| \leq \sigma |f(x) - f^{\textbackslash i}(x)| = \sigma |\Delta f(x) |
		\]
		Using \eqref{e:cauchy_schwarz},we have 
		\[
		| l(f,z) - l(f^{\textbackslash i},z)| \leq \sigma \kappa \|\Delta f\|_\clH = \frac{\kappa^2 \sigma^2}{2 \lambda N}
		\]
	\end{proof}
\end{theorem}

One additional condition that is required is to bound the loss function. If we have an apriori bound on the target values $y_i$, then the boundedness condition is satisfied.


Let $f_\pr$ denote the \textit{regression function},
\[
f_\pr := \argmin \Expect_{\pr_{XY}}[(f(x) - y)^2]
\]

For a more general problem
\[
f_\pr := \argmin_f \Expect_{\pr_{XY}}[c(f(x), y)]
\]
\[
f_S := \argmin_{f \in \clH} \frac{1}{N} \sum_{i=1}^N c(f(x_i), y_i) + \lambda \|f\|^2_\clH 
\]