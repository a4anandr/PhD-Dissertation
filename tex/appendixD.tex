\chapter{Generalization Error Bounds for least-squares regression on RKHS }%
\label{a:bousquet}
Here, we state some results on generalization error bounds for least-squares regression on RKHS obtained in \cite{boueli01} using the notions of stability of the algorithm.
Extensions to include ERM with gradient terms in the loss function is part of future work. 

Let $Z = X \times Y$ be the space of inputs and outputs and let $S = \{z_1 = (x_1, y_1), \cdots,z_N = (x_N,y_N)\}$ be a dataset of size $N$ drawn i.i.d. from an unknown distribution $\pr_{XY}$. Let $f:X \to Y$ be a function that maps from the input space $X$ to output space $Y$. A learning algorithm tries to learn the function $f_S$ from the given dataset $S$. 

The generalization error or expected risk is defined as:
\[
R(f) := \Expect_{z \sim \pr_{XY}} [c(f,z)]
\]
The empirical error of a function $f$ measured on the training set $S$ is:
\[
R_N(f) := \frac{1}{N}\sum_{i=1}^N c(f,z_i)
\]
Our aim is to obtain bounds on the random variable $R(f_S) - R_N(f_S)$. 

If $S = \{z_1,\dots, z_{i-1},z_i,z_{i+1},\dots z_N\}$, let us define a modified training set $S_i :=\{z_1,\dots, z_{i-1},z'_i,z_{i+1},\dots,z_N\}$ where the $i^{th}$ training sample $z_i$ is replaced by $z'_i$.

Uniform stability: A notion of uniform stability is defined for the algorithm as follows. If the training set $S$ is defined as above and $S_i$ be the training set where $i^{th}$ sample is removed, then the algorithm is $\beta$-stable if the following holds:
\[
\|c(f_S, z ) - c(f_{S_i},z)\|_{\infty} \leq \beta, \qquad \forall S \in Z^N, \forall z'_i, z \in Z,
\]
where $c(\cdot,\cdot)$ denotes the loss function. 
This condition implies stability of the algorithm in the sense that if a training sample from the original set is replaced by a new sample, the difference in cost is smaller than some constant $\beta$. 

\begin{lemma}
	For any symmetric learning algorithm we have for all $1\leq i \leq N$:
	\[
	\Expect_{S \sim \pr^N_{XY}} [R(f_S) - R_N(f_S)] = \Expect_{S,z'_i \sim \pr^{N+1}_{XY}} [c(f_S,z'_i) - c(f_{S_i}, z'_i)]
	\]
	\begin{proof}
		\[
		\Expect_{S \sim \pr^N_{XY}} [R_N(f_S)] = \frac{1}{N} \sum_{i=1}^N \Expect_{S \sim \pr^N_{XY}} [c(f_S, z_i)] = \Expect_{S \sim \pr^N_{XY}} [c(f_S, z_i)], \, \forall i \in \{1,\dots, N\}
		\]
		The above is true by symmetry and the i.i.d assumption. Now by simply renaming $z_i$ as $z'_i$,
		\[
		\Expect_{S \sim \pr^N_{XY}}[R_N(f_S)] = \Expect_{S_i \sim \pr^N_{XY}}[c(f_{S_i}, z'_i)]
		\]
		The expected risk term can be written as, 
		\[
		\Expect_{S \sim \pr^N_{XY}}[R(f_S)] = \Expect_{S,z'_i \sim \pr^{N+1}_{XY}} [c(f_S, z'_i)]
		\]
		Using this and the fact that the algorithm is $\beta$-stable, 
		\[
		\begin{aligned}
		\Expect_{S \sim \pr^N_{XY}} [R(f_S) - R_N(f_S)] &= \Expect_{S,z'_i\sim \pr^{N+1}_{XY}}[c(f_S,z'_i) - c(f_{S_i},z'_i)]\\
		&\leq \Expect_{S,z'_i\sim \pr^{N+1}_{XY}}[\beta] \\
		& = \beta
		\end{aligned}
		\]
	\end{proof}
\end{lemma}
Now, the next step is to prove that our algorithm of interest is $\beta$-stable for some value of $\beta$. 

\section{Application to Regularization in Hilbert Spaces}
Let $\clH$ be an RKHS induced by a kernel function $K$. Let us assume that $K$ is continuous and bounded, so that
\[
\kappa := \sup_{x \in X} \sqrt{K(x,x)} < \infty
\]
and therefore, by Cauchy-Schwarz inequality,
\begin{equation}
|f(x)| \leq \kappa \|f\|_\clH, \, \forall x \in X, \forall f \in \clH 
\label{e:cauchy_schwarz}
\end{equation}

\noindent $\sigma$-admissibility: A cost function $c(f(x),y)$ defined on $\clH \times Y$ is $\sigma$-admissible with respect to $\clH$ if $c$ is convex with respect to its first argument and the following condition holds
\[
|c(y_1, y') - c(y_2, y')| \leq \sigma |y_1 - y_2|, \, \forall y_1,y_2 \in \mathcal{D}, \forall y' \in Y,
\]
where $\mathcal{D} = \{y: \exists f \in \clH, \exists x \in X, f(x) = y\}$ is the domain of the first argument of $c$.  

\noindent \begin{theorem}
	If $l(f,z) = c(f(x),y)$ is $\sigma$-admissible with respect to $\clH$, then the learning algorithm defined by 
	\[
	A_S = \argmin_{g \in \clH} \frac{1}{N} \sum_{i=1}^N l(g, z_i) + \lambda \|g\|^2_\clH
	\]
	has uniform stability $\beta$ with respect to $l$ with
	\[
	\beta \leq \frac{\kappa^2 \sigma^2}{2 \lambda N}
	\]
\end{theorem}
The proof uses Lemma 20 in the paper that gives bounds on a general regularization term $N(g)$ that appears in the ERM. Here, we just state the result without the proof. 
